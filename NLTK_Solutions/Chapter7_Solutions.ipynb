{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "Write a tag pattern to match noun phrases containing plural head nouns, e.g. \"many/JJ researchers/NNS\", \"two/CD weeks/NNS\", \"both/DT new/JJ positions/NNS\". Try to do this by generalizing the tag pattern that handled singular noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "patern = r\"<DT>?(<JJ>|<CD>)*<NNS>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "Pick one of the three chunk types in the CoNLL corpus. Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences that make up this kind of chunk. Develop a simple chunker using the regular expression chunker nltk.RegexpParser. Discuss any tag sequences that are difficult to chunk reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  But/CC\n",
      "  analysts/NNS\n",
      "  (VP reckon/VBP)\n",
      "  underlying/VBG\n",
      "  support/NN\n",
      "  for/IN\n",
      "  sterling/NN\n",
      "  (VP has/VBZ been/VBN eroded/VBN)\n",
      "  by/IN\n",
      "  the/DT\n",
      "  chancellor/NN\n",
      "  's/POS\n",
      "  failure/NN\n",
      "  (VP to/TO announce/VB)\n",
      "  any/DT\n",
      "  new/JJ\n",
      "  policy/NN\n",
      "  measures/NNS\n",
      "  in/IN\n",
      "  his/PRP$\n",
      "  Mansion/NNP\n",
      "  House/NNP\n",
      "  speech/NN\n",
      "  last/JJ\n",
      "  Thursday/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.conll2000.chunked_sents(\"train.txt\", chunk_types=[\"VP\"])[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = nltk.RegexpParser(\"VP: {<TO>?<HV.*>?<BE.*>?<VB.*>+}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  ``/``\n",
      "  Only/RB\n",
      "  a/AT\n",
      "  relative/JJ\n",
      "  handful/NN\n",
      "  of/IN\n",
      "  such/JJ\n",
      "  reports/NNS\n",
      "  (VP was/BEDZ received/VBN)\n",
      "  ''/''\n",
      "  ,/,\n",
      "  the/AT\n",
      "  jury/NN\n",
      "  (VP said/VBD)\n",
      "  ,/,\n",
      "  ``/``\n",
      "  considering/IN\n",
      "  the/AT\n",
      "  widespread/JJ\n",
      "  interest/NN\n",
      "  in/IN\n",
      "  the/AT\n",
      "  election/NN\n",
      "  ,/,\n",
      "  the/AT\n",
      "  number/NN\n",
      "  of/IN\n",
      "  voters/NNS\n",
      "  and/CC\n",
      "  the/AT\n",
      "  size/NN\n",
      "  of/IN\n",
      "  this/DT\n",
      "  city/NN\n",
      "  ''/''\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(chunker.parse(nltk.corpus.brown.tagged_sents()[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    " An early definition of chunk was the material that occurs between chinks. Develop a chunker that starts by putting the whole sentence in a single chunk, and then does the rest of its work solely by chinking. Determine which tags (or tag sequences) are most likely to make up chinks with the help of your own utility program. Compare the performance and simplicity of this approach relative to a chunker based entirely on chunk rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "NP:\n",
    "{<.*>+}\n",
    "}<VB.*|IN|\\W*|BE.*|HV.*>+{\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  ``/``\n",
      "  (NP Only/RB a/AT relative/JJ handful/NN)\n",
      "  of/IN\n",
      "  (NP such/JJ reports/NNS)\n",
      "  was/BEDZ\n",
      "  received/VBN\n",
      "  ''/''\n",
      "  ,/,\n",
      "  (NP the/AT jury/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  considering/IN\n",
      "  (NP the/AT widespread/JJ interest/NN)\n",
      "  in/IN\n",
      "  (NP the/AT election/NN)\n",
      "  ,/,\n",
      "  (NP the/AT number/NN)\n",
      "  of/IN\n",
      "  (NP voters/NNS and/CC the/AT size/NN)\n",
      "  of/IN\n",
      "  (NP this/DT city/NN)\n",
      "  ''/''\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(cp.parse(nltk.corpus.brown.tagged_sents()[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "Write a tag pattern to cover noun phrases that contain gerunds, e.g. \"the/DT receiving/VBG end/NN\", \"assistant/NN managing/VBG editor/NN\". Add these patterns to the grammar, one per line. Test your work using some tagged sentences of your own devising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "NP:\n",
    "{<DT>?(<NN.*>?<VBG>|<JJ>*)<NN.*>+}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT receiving/VBG end/NN)\n",
      "  (NP the/DT money/NN making/VBG corporations/NNS))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"receiving\", \"VBG\"), (\"end\", \"NN\"), \n",
    "            (\"the\", \"DT\"), (\"money\", \"NN\"), (\"making\", \"VBG\"), (\"corporations\", \"NNS\")]\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "Write one or more tag patterns to handle coordinated noun phrases, e.g. \"July/NNP and/CC August/NNP\", \"all/DT your/PRP$ managers/NNS and/CC supervisors/NNS\", \"company/NN courts/NNS and/CC adjudicators/NNS\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "NP:\n",
    "{<DT>?<PRP\\$>?(<NN.*>?<VBG>|<JJ>*)?<NN.*>+(<CC><DT>?<PRP$>?(<NN.*>?<VBG>|<JJ>*)?<NN.*>+)*}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP July/NNP and/CC August/NNP))\n",
      "(S (NP all/DT your/PRP$ managers/NNS and/CC supervisor/NNS))\n",
      "(S (NP company/NN courts/NNS and/CC adjudicators/NNS))\n"
     ]
    }
   ],
   "source": [
    "sentence1 = [(\"July\", \"NNP\"), (\"and\", \"CC\"), (\"August\", \"NNP\")]\n",
    "sentence2 = [(\"all\", \"DT\"), (\"your\", \"PRP$\"), (\"managers\", \"NNS\"), (\"and\", \"CC\"), (\"supervisor\", \"NNS\")]\n",
    "sentence3 = [(\"company\", \"NN\"), (\"courts\", \"NNS\"), (\"and\", \"CC\"), (\"adjudicators\", \"NNS\")]\n",
    "print(cp.parse(sentence1))\n",
    "print(cp.parse(sentence2))\n",
    "print(cp.parse(sentence3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\n",
    "Carry out the following evaluation tasks for any of the chunkers you have developed earlier. (Note that most chunking corpora contain some internal inconsistencies, such that any reasonable rule-based approach will produce errors.)\n",
    "\n",
    "    a. Evaluate your chunker on 100 sentences from a chunked corpus, and report the precision, recall and F-measure.\n",
    "    b. Use the chunkscore.missed() and chunkscore.incorrect() methods to identify the errors made by your chunker. Discuss.\n",
    "    c. Compare the performance of your chunker to the baseline chunker discussed in the evaluation section of this chapter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_sents = nltk.corpus.conll2000.chunked_sents()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "NP:\n",
    "{<DT>?<PRP\\$>?(<NN.*>?<VBG>|<JJ>*)?<NN.*>+(<CC><DT>?<PRP$>?(<NN.*>?<VBG>|<JJ>*)?<NN.*>+)*}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  56.0%%\n",
      "    Precision:     74.6%%\n",
      "    Recall:        35.5%%\n",
      "    F-Measure:     48.1%%\n"
     ]
    }
   ],
   "source": [
    "print(cp.evaluate(chunked_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk.util import ChunkScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = ChunkScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = [cp.parse(sent) for sent in chunked_sents]\n",
    "cs.score(guess, chunked_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ImmutableTree('S', [('Chancellor', 'NNP'), ImmutableTree('PP', [('of', 'IN')]), ImmutableTree('NP', [('the', 'DT'), ('Exchequer', 'NNP')]), ImmutableTree('NP', [('Nigel', 'NNP'), ('Lawson', 'NNP')]), ImmutableTree('NP', [(\"'s\", 'POS'), ('restated', 'VBN'), ('commitment', 'NN')]), ImmutableTree('PP', [('to', 'TO')]), ImmutableTree('NP', [('a', 'DT'), ('firm', 'NN'), ('monetary', 'JJ'), ('policy', 'NN')]), ImmutableTree('VP', [('has', 'VBZ'), ('helped', 'VBN'), ('to', 'TO'), ('prevent', 'VB')]), ImmutableTree('NP', [('a', 'DT'), ('freefall', 'NN')]), ImmutableTree('PP', [('in', 'IN')]), ImmutableTree('NP', [('sterling', 'NN')]), ImmutableTree('PP', [('over', 'IN')]), ImmutableTree('NP', [('the', 'DT'), ('past', 'JJ'), ('week', 'NN')]), ('.', '.')]), ImmutableTree('S', [ImmutableTree('PP', [('In', 'IN')]), ImmutableTree('NP', [('an', 'DT'), ('Oct.', 'NNP'), ('19', 'CD'), ('letter', 'NN')]), ImmutableTree('PP', [('to', 'TO')]), ImmutableTree('NP', [('A.P.', 'NNP'), ('Green', 'NNP')]), ImmutableTree('NP', [(\"'s\", 'POS'), ('board', 'NN')]), (',', ','), ImmutableTree('NP', [('East', 'NNP'), ('Rock', 'NNP')]), ImmutableTree('VP', [('said', 'VBD')]), ImmutableTree('NP', [('the', 'DT'), ('offer', 'NN')]), ImmutableTree('VP', [('is', 'VBZ')]), ('subject', 'NN'), ImmutableTree('PP', [('to', 'TO')]), ImmutableTree('NP', [('the', 'DT'), ('signing', 'NN')]), ImmutableTree('PP', [('of', 'IN')]), ImmutableTree('NP', [('a', 'DT'), ('merger', 'NN'), ('agreement', 'NN')]), ImmutableTree('PP', [('by', 'IN')]), ('no', 'DT'), ('later', 'RB'), ImmutableTree('PP', [('than', 'IN')]), ImmutableTree('NP', [('Oct.', 'NNP'), ('31', 'CD')]), ('.', '.')]), ImmutableTree('S', [ImmutableTree('NP', [('Analysts', 'NNS')]), ImmutableTree('VP', [('agree', 'VBP')]), ImmutableTree('NP', [('there', 'EX')]), ImmutableTree('VP', [('is', 'VBZ')]), ImmutableTree('NP', [('little', 'JJ')]), ImmutableTree('VP', [('holding', 'NN')]), ImmutableTree('NP', [('sterling', 'NN')]), ('firm', 'NN'), ImmutableTree('PP', [('at', 'IN')]), ImmutableTree('NP', [('the', 'DT'), ('moment', 'NN')]), ('other', 'JJ'), ImmutableTree('PP', [('than', 'IN')]), ImmutableTree('NP', [('Mr.', 'NNP'), ('Lawson', 'NNP')]), ImmutableTree('NP', [(\"'s\", 'POS'), ('promise', 'NN')]), ('that', 'IN'), ImmutableTree('NP', [('rates', 'NNS')]), ImmutableTree('VP', [('will', 'MD'), ('be', 'VB'), ('pushed', 'VBN')]), ('higher', 'JJR'), ('if', 'IN'), ('necessary', 'JJ'), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "print(cs.incorrect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ImmutableTree('S', [ImmutableTree('NP', [('Analysts', 'NNS')]), ImmutableTree('VP', [('agree', 'VBP')]), ImmutableTree('NP', [('there', 'EX')]), ImmutableTree('VP', [('is', 'VBZ')]), ImmutableTree('NP', [('little', 'JJ')]), ImmutableTree('VP', [('holding', 'NN')]), ImmutableTree('NP', [('sterling', 'NN')]), ImmutableTree('NP', [('firm', 'NN')]), ImmutableTree('PP', [('at', 'IN')]), ImmutableTree('NP', [('the', 'DT'), ('moment', 'NN')]), ('other', 'JJ'), ImmutableTree('PP', [('than', 'IN')]), ImmutableTree('NP', [('Mr.', 'NNP'), ('Lawson', 'NNP')]), ImmutableTree('NP', [(\"'s\", 'POS'), ('promise', 'NN')]), ('that', 'IN'), ImmutableTree('NP', [('rates', 'NNS')]), ImmutableTree('VP', [('will', 'MD'), ('be', 'VB'), ('pushed', 'VBN')]), ('higher', 'JJR'), ('if', 'IN'), ('necessary', 'JJ'), ('.', '.')]), ImmutableTree('S', [ImmutableTree('NP', [('Chancellor', 'NNP')]), ImmutableTree('PP', [('of', 'IN')]), ImmutableTree('NP', [('the', 'DT'), ('Exchequer', 'NNP')]), ImmutableTree('NP', [('Nigel', 'NNP'), ('Lawson', 'NNP')]), ImmutableTree('NP', [(\"'s\", 'POS'), ('restated', 'VBN'), ('commitment', 'NN')]), ImmutableTree('PP', [('to', 'TO')]), ImmutableTree('NP', [('a', 'DT'), ('firm', 'NN'), ('monetary', 'JJ'), ('policy', 'NN')]), ImmutableTree('VP', [('has', 'VBZ'), ('helped', 'VBN'), ('to', 'TO'), ('prevent', 'VB')]), ImmutableTree('NP', [('a', 'DT'), ('freefall', 'NN')]), ImmutableTree('PP', [('in', 'IN')]), ImmutableTree('NP', [('sterling', 'NN')]), ImmutableTree('PP', [('over', 'IN')]), ImmutableTree('NP', [('the', 'DT'), ('past', 'JJ'), ('week', 'NN')]), ('.', '.')]), ImmutableTree('S', [ImmutableTree('PP', [('In', 'IN')]), ImmutableTree('NP', [('an', 'DT'), ('Oct.', 'NNP'), ('19', 'CD'), ('letter', 'NN')]), ImmutableTree('PP', [('to', 'TO')]), ImmutableTree('NP', [('A.P.', 'NNP'), ('Green', 'NNP')]), ImmutableTree('NP', [(\"'s\", 'POS'), ('board', 'NN')]), (',', ','), ImmutableTree('NP', [('East', 'NNP'), ('Rock', 'NNP')]), ImmutableTree('VP', [('said', 'VBD')]), ImmutableTree('NP', [('the', 'DT'), ('offer', 'NN')]), ImmutableTree('VP', [('is', 'VBZ')]), ImmutableTree('NP', [('subject', 'NN')]), ImmutableTree('PP', [('to', 'TO')]), ImmutableTree('NP', [('the', 'DT'), ('signing', 'NN')]), ImmutableTree('PP', [('of', 'IN')]), ImmutableTree('NP', [('a', 'DT'), ('merger', 'NN'), ('agreement', 'NN')]), ImmutableTree('PP', [('by', 'IN')]), ('no', 'DT'), ('later', 'RB'), ImmutableTree('PP', [('than', 'IN')]), ImmutableTree('NP', [('Oct.', 'NNP'), ('31', 'CD')]), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "print(cs.missed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8\n",
    "Develop a chunker for one of the chunk types in the CoNLL corpus using a regular-expression based chunk grammar RegexpChunk. Use any combination of rules for chunking, chinking, merging or splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000 as con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP This/DT)\n",
      "  has/VBZ\n",
      "  increased/VBN\n",
      "  (NP the/DT risk/NN)\n",
      "  of/IN\n",
      "  (NP the/DT government/NN)\n",
      "  being/VBG\n",
      "  forced/VBN\n",
      "  to/TO\n",
      "  increase/VB\n",
      "  (NP base/NN rates/NNS)\n",
      "  to/TO\n",
      "  (NP 16/CD %/NN)\n",
      "  from/IN\n",
      "  (NP their/PRP$ current/JJ 15/CD %/NN level/NN)\n",
      "  to/TO\n",
      "  defend/VB\n",
      "  (NP the/DT pound/NN)\n",
      "  ,/,\n",
      "  (NP economists/NNS)\n",
      "  and/CC\n",
      "  (NP foreign/JJ exchange/NN market/NN analysts/NNS)\n",
      "  say/VBP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "chunked_sents = con.chunked_sents(chunk_types=[\"NP\"])[3]\n",
    "print(chunked_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "NP:\n",
    "{(<DT>?<PRP\\$>?(<NN.*>?<VBG>|<JJ|CD>*)?<NN.*>+(<CC><DT>?<PRP$>?(<NN.*>?<VBG>|<JJ|CD>*)?<NN.*>+)*)|<DT>}\n",
    "}<VB.*|IN|\\W*|BE.*|HV.*>+{\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP This/DT)\n",
      "  has/VBZ\n",
      "  increased/VBN\n",
      "  (NP the/DT risk/NN)\n",
      "  of/IN\n",
      "  (NP the/DT government/NN)\n",
      "  being/VBG\n",
      "  forced/VBN\n",
      "  to/TO\n",
      "  increase/VB\n",
      "  (NP base/NN rates/NNS)\n",
      "  to/TO\n",
      "  (NP 16/CD %/NN)\n",
      "  from/IN\n",
      "  (NP their/PRP$ current/JJ 15/CD %/NN level/NN)\n",
      "  to/TO\n",
      "  defend/VB\n",
      "  (NP the/DT pound/NN)\n",
      "  ,/,\n",
      "  (NP\n",
      "    economists/NNS\n",
      "    and/CC\n",
      "    foreign/JJ\n",
      "    exchange/NN\n",
      "    market/NN\n",
      "    analysts/NNS)\n",
      "  say/VBP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.parse(con.tagged_sents()[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9\n",
    "Sometimes a word is incorrectly tagged, e.g. the head noun in \"12/CD or/CC so/RB cases/VBZ\". Instead of requiring manual correction of tagger output, good chunkers are able to work with the erroneous output of taggers. Look for other examples of correctly chunked noun phrases with incorrect tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10\n",
    "The bigram chunker scores about 90% accuracy. Study its errors and try to work out why it doesn't get 100% accuracy. Experiment with trigram chunking. Are you able to improve the performance any more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)]\\\n",
    "                     for sent in train_sents]\n",
    "        self.t1 = nltk.UnigramTagger(train_data)\n",
    "        self.tagger = nltk.BigramTagger(train_data, backoff=self.t1)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\\\n",
    "                    in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = con.chunked_sents(\"train.txt\")\n",
    "test_sents = con.chunked_sents(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BigramChunker(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.5%%\n",
      "    Precision:     81.1%%\n",
      "    Recall:        86.4%%\n",
      "    F-Measure:     83.7%%\n"
     ]
    }
   ],
   "source": [
    "print(bc.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramChunker(nltk.ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)]\\\n",
    "                     for sent in train_sents]\n",
    "        self.t1 = nltk.UnigramTagger(train_data)\n",
    "        self.t2 = nltk.BigramTagger(train_data, backoff=self.t1)\n",
    "        self.tagger = nltk.TrigramTagger(train_data, backoff=self.t2)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\\\n",
    "                    in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  88.8%%\n",
      "    Precision:     80.9%%\n",
      "    Recall:        85.6%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "tc = TrigramChunker(train_sents)\n",
    "print(tc.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
