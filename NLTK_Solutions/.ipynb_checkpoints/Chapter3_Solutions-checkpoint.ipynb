{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8\n",
    "Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML\n",
    "markup removed. Use from urllib import request and then\n",
    "request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_text(url):\n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    raw = BeautifulSoup(html).get_text()\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18\n",
    " Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in\n",
    "English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order.\n",
    "Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = gutenberg.words(gutenberg.fileids()[-1])\n",
    "whs = ['what', 'when', 'who', 'where', 'why', 'how']\n",
    "wh_list = [w for w in words if w.lower() in whs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19\n",
    " Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space\n",
    "character, and a positive integer, e.g. fuzzy 53 . Read the file into a Python list using\n",
    "open(filename).readlines() . Next, break each line into its two fields using split() , and convert the number\n",
    "into an integer using int() . The result should be a list of the form: [['fuzzy', 53], ...] ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20\n",
    "Write code to access a favorite webpage and extract some text from it. For example, access a weather site and\n",
    "extract the forecast top temperature for your town or city today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://weather.naver.com/\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "raw = BeautifulSoup(html).get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21\n",
    " Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on\n",
    "that webpage. In order to do this, extract all substrings consisting of lowercase letters (using re.findall() ) and\n",
    "remove any items from this set that occur in the Words Corpus ( nltk.corpus.words ). Try to categorize these\n",
    "words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown(url):\n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    raw = BeautifulSoup(html).get_text()\n",
    "    extracted = set(re.findall(\"[a-z]+\", raw))\n",
    "    targets = [w for w in extracted if w not in words.words()]\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['package',\n",
       " 'packbuilder',\n",
       " 'packcloth',\n",
       " 'packer',\n",
       " 'packery',\n",
       " 'packet',\n",
       " 'packhouse',\n",
       " 'packless',\n",
       " 'packly',\n",
       " 'packmaker']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.words()[136838:136838+10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22\n",
    " Examine the results of processing the URL http://news.bbc.co.uk/ using the regular expressions suggested\n",
    "above. You will see that there is still a fair amount of non-textual data there, particularly Javascript commands.\n",
    "You may also find that sentence breaks have not been properly preserved. Define further regular expressions that\n",
    "improve the extraction of text from this web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = url_text(\"http://news.bbc.co.uk/ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\"[a-zA-Z]+\", \"[A-Z][a-z]*\", \"p[aeiou]{,2}t\", \"\\d+(\\.\\d+)?\", \"([^aeiou][aeiou][^aeiou])+\", \"\\w+|[^\\w\\s]+ \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23\n",
    "Are you able to write a regular expression to tokenize text in such a way that the word don't is tokenized into do\n",
    "and n't? Explain why this regular expression won't work: « n't|\\w+ »."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don', 't', 'fly']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"\\w+(?:n't)?\", \"don't fly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24\n",
    "Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3 , i → 1 , o\n",
    "→ 0 , l → | , s → 5 , . → 5w33t! , ate → 8 . Normalize the text to lowercase before converting it. Add more\n",
    "substitutions of your own. Now try to map s to two different values: $ for word-initial s , and 5 for word-internal s ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"ate\", \"8\", text)\n",
    "    text = re.sub(\"e\", \"3\", text)\n",
    "    text = re.sub(\"i\", \"1\", text)\n",
    "    text = re.sub(\"o\", \"0\", text)\n",
    "    text = re.sub(\"l\", \"|\", text)\n",
    "    text = re.sub(\"s\", \"5\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25\n",
    " Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any\n",
    "consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g. string →\n",
    "ingstray, idle → idleay. http://en.wikipedia.org/wiki/Pig_Latin\n",
    "- a.Write a function to convert a word to Pig Latin. \n",
    "- b.Write code that converts text, instead of individual words. \n",
    "- c.Extend it further to preserve capitalization, to keep qu together (i.e. so that quiet becomes ietquay ), and to\n",
    "detect when y is used as a consonant (e.g. yellow ) vs a vowel (e.g. style )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pigLatin(word):\n",
    "    match = re.match(\"[^aeiou]+\", word)\n",
    "    if match:\n",
    "        word = re.sub(\"^\" + match.group(), \"\", word) + match.group()\n",
    "    return word + \"ay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pigConvert(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [pigLatin(w) for w in text]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pigLatin2(word):\n",
    "    match = re.match(\"[^aeiouy]+|^qu|(^y)\", word)\n",
    "    if match:\n",
    "        word = re.sub(\"^\" + match.group(), \"\", word) + match.group()\n",
    "    return word + \"ay\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26\n",
    "Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of\n",
    "words, and create a vowel bigram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./corpus/spanish.txt\", 'r') as f:\n",
    "    text = f.read()\n",
    "vowels = set(re.findall(\"[aáeéiíoóuúü]{2,}\", text))\n",
    "vowel_bigrams = [(w[0], w[1]) for w in vowels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27\n",
    " Python's random module includes a function choice() which randomly chooses an item from a sequence, e.g.\n",
    "choice(\"aehh \") will produce one of four possible characters, with the letter h being twice as frequent as the\n",
    "others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the\n",
    "string \"aehh \" , and put this expression inside a call to the ''.join() function, to concatenate them into one long\n",
    "string. You should get a result that looks like uncontrolled sneezing or maniacal laughter:\n",
    "he haha ee heheeh eha . Use split() and join() again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ehhhaehehh ehhhaheahehhhh hhaea aeaehhh haaheaahaah haaaeaha ahaa eeae h h hhhahah hhhhh hhehe hahehhhh h hhh a h e a hhahhahhaha a hhhh ehhhehee he e ee ah heheahehhhahhhaa ae he h h eaeah hhh hhh h ehhh h ha hhahhhhah aeeh e eaaeaahhh haa hh ah h ahh ae eehah ahhah ehhhahhh hahehhehahaah hhha ah hhheaeehhaeehae eh haeahehhehe ee hehhha aaaaeaeehea heh hhhhhha hhhh hahea aeehe hhhaeheha aehh ahhhhe hah hahah eh ahaaeeh hhe hheaahha aahhhh h aaae hahh h hh hhhhea hah h he hhh\n"
     ]
    }
   ],
   "source": [
    "def generator(text):\n",
    "    for _ in range(500):\n",
    "        yield random.choice(text)\n",
    "print(\" \".join(\"\".join(generator(\"aehh \")).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28\n",
    "Consider the numeric expressions in the following sentence from the MedLine Corpus: The corresponding free\n",
    "cortisol fractions in these sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively. Should we say that the\n",
    "numeric expression 4.53 +/- 0.15% is three words? Or should we say that it's a single compound word? Or should\n",
    "we say that it is actually nine words, since it's read \"four point five three, plus or minus zero point fifteen percent\"?\n",
    "Or should we say that it's not a \"real\" word at all, since it wouldn't appear in any dictionary? Discuss these\n",
    "different possibilities. Can you think of application domains that motivate at least two of these answers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29\n",
    "Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of 29.\n",
    "appropriate difficulty for language learners. Let us define μ w to be the average number of letters per word, and μ s\n",
    "to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the\n",
    "text is defined to be: 4.71 μ w + 0.5 μ s - 21.43 . Compute the ARI score for various sections of the Brown\n",
    "Corpus, including section f (lore) and j (learned). Make use of the fact that nltk.corpus.brown.words()\n",
    "produces a sequence of words, while nltk.corpus.brown.sents() produces a sequence of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARI(category):\n",
    "    num_let = [len(w) for fileid in brown.fileids(categories=category) for w in brown.words(fileid)]\n",
    "    mu_w = sum(num_let) / len(num_let)\n",
    "    \n",
    "    num_w = [len(s) for fileid in brown.fileids(categories=category) for s in brown.sents(fileid)]\n",
    "    mu_s = sum(num_w) / len(num_w)\n",
    "    \n",
    "    return 4.71*mu_w + 0.5*mu_s - 21.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.254756197101155"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARI('lore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.926007043317348"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARI(\"learned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30\n",
    "Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing\n",
    "with the Lancaster Stemmer and see if you observe any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = gutenberg.words('austen-emma.txt')\n",
    "portStems = [PorterStemmer().stem(w) for w in text]\n",
    "lancStems = [LancasterStemmer().stem(w) for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'I', 'chapter', 'I', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich']\n",
      "['[', 'emm', 'by', 'jan', 'aust', '1816', ']', 'volum', 'i', 'chapt', 'i', 'emm', 'woodh', ',', 'handsom', ',', 'clev', ',', 'and', 'rich']\n"
     ]
    }
   ],
   "source": [
    "print(portStems[:20])\n",
    "print(lancStems[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31\n",
    "Define the variable saying to contain the list\n",
    "['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
    "'is', 'said', 'than', 'done', '.'] . Process this list using a for loop, and store the length of each word in a\n",
    "new list lengths . Hint: begin by assigning the empty list to lengths , using lengths = [] . Then each time\n",
    "through the loop, use append() to add another length value to the list. Now do the same thing using a list\n",
    "comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']\n",
    "lengths = []\n",
    "for x in variable:\n",
    "    lengths.append(len(x))\n",
    "lengths = [len(x) for x in variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32\n",
    "Define a variable silly to contain the string:\n",
    "'newly formed bland ideas are inexpressible in an infuriating\n",
    "way' . (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to\n",
    "Chomsky's famous nonsense phrase, colorless green ideas sleep furiously according to Wikipedia). Now write\n",
    "code to perform the following tasks:\n",
    "\n",
    "- a. Split silly into a list of strings, one per word, using Python's split() operation, and save this to a variable\n",
    "called bland .\n",
    "- b. Extract the second letter of each word in silly and join them into a string, to get 'eoldrnnnna' . \n",
    "- c. Combine the words in bland back into a single string, using join() . Make sure the words in the resulting\n",
    "string are separated with whitespace.\n",
    "- d. Print the words of silly in alphabetical order, one per line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "silly =  'newly formed bland ideas are inexpressible in an infuriating way' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']\n"
     ]
    }
   ],
   "source": [
    "bland = silly.split()\n",
    "print(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eoldrnnnna'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(w[1] for w in bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newly formed bland ideas are inexpressible in an infuriating way'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(sorted(silly.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33\n",
    " The index() function can be used to look up items in sequences. For example, 'inexpressible'.index('e')\n",
    "tells us the index of the first position of the letter e.\n",
    "- a. What happens when you look up a substring, e.g. 'inexpressible'.index('re') ? \n",
    "- b. Define a variable words containing a list of words. Now use words.index() to look up the position of an\n",
    "individual word.\n",
    "- c. Define a variable silly as in the exercise above. Use the index() function in combination with list slicing\n",
    "to build a list phrase consisting of all the words up to (but not including) in in silly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 34\n",
    "Write code to convert nationality adjectives like Canadian and Australian to their corresponding nouns Canada\n",
    "and Australia (see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 39\n",
    "Read the Wikipedia entry on Soundex. Implement this algorithm in Python. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_to_digit = {\"b\" : 1, \"f\" : 1, \"p\" : 1, \"v\" : 1, \"c\" : 2,   \"g\" : 2,   \"j\" : 2,   \"k\" : 2,   \"q\" : 2,   \"s\" : 2,   \"x\" : 2,  \n",
    "\"z\" : 2, \"d\" : 3, \"t\" : 3, \"l\" : 4, \"m\" : 5, \"n\" : 5, \"r\" : 6, \"a\" : 0, \"e\" : 0, \"i\" : 0, \"o\" : 0, \"u\" : 0, \"y\" : 0, \"h\" : 0, \"w\" : 0}\n",
    "\n",
    "def soundex(word):\n",
    "    first = word[0].lower()\n",
    "    rest = re.sub(\"[aeiouyhw]\", \"\", word[1:])\n",
    "    for l in rest:\n",
    "        if con_to_digit[first] == con_to_digit[l]:\n",
    "            rest = rest[1:]\n",
    "        else: break\n",
    "    for l in rest:\n",
    "        if first[-1] != con_to_digit[l]:\n",
    "            first += str(con_to_digit[l])\n",
    "        if len(first) == 4:\n",
    "            break\n",
    "    return \"{:0<4s}\".format(first).title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R150'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soundex(\"Rubin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41\n",
    "Rewrite the following nested loop as a nested list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsequences = set(''.join([char for char in word if char in 'aeiou']) for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42\n",
    "Use WordNet to create a semantic index for a text collection. Extend the concordance search program in 3.6,\n",
    "indexing each word using the offset of its first synset, e.g. wn.synsets('dog')[0].offset (and optionally the\n",
    "offset of some of its ancestors in the hypernym hierarchy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2084071"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')[0].offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
