{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000 as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(cl.chunked_sents(\"train.txt\", chunk_types=['NP'])[99])  \n",
    "# chunk_types parameter can narrow down the range of chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(\"\")\n",
    "test_sents = cl.chunked_sents(\"test.txt\", chunk_types=['NP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means 43.4% of the words are not in NP chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)]\\\n",
    "                     for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\\\n",
    "                    in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = cl.chunked_sents(\"train.txt\", chunk_types=['NP'])\n",
    "test_sents = cl.chunked_sents(\"test.txt\", chunk_types=['NP'])\n",
    "# unigram_chunker = UnigramChunker(train_sents)\n",
    "# print(unigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'), (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'), ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'), ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'), ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'), ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'), ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'), ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'), ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'), ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# To check what this chunker actually does\n",
    "postags = sorted(set(pos for sent in train_sents for (word, pos)\\\n",
    "                    in sent.leaves()))\n",
    "print(unigram_chunker.tagger.tag(postags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)]\\\n",
    "                     for sent in train_sents]\n",
    "        self.t1 = nltk.UnigramTagger(train_data)\n",
    "        self.tagger = nltk.BigramTagger(train_data, backoff=self.t1)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\\\n",
    "                    in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_chunker = BigramChunker(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.4%%\n",
      "    Precision:     82.3%%\n",
      "    Recall:        87.0%%\n",
      "    F-Measure:     84.6%%\n"
     ]
    }
   ],
   "source": [
    "print(bigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append((featureset, tag))\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.MaxentClassifier.train(\\\n",
    "            train_set, algorithm='megam', trace=0)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w, t), c) \\\n",
    "            for w, t, c in nltk.chunk.tree2conlltags(sent)]\\\n",
    "            for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w, t, c) for ((w, t), c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://users.umiacs.umd.edu/~hal/megam/index.html>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5685649df20f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"pos\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mchunker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConsecutiveNPChunker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-78ab67617484>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_sents)\u001b[0m\n\u001b[0;32m     26\u001b[0m             for w, t, c in nltk.chunk.tree2conlltags(sent)]\\\n\u001b[0;32m     27\u001b[0m             for sent in train_sents]\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConsecutiveNPChunkTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-78ab67617484>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_sents)\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         self.classifier = nltk.MaxentClassifier.train(\\\n\u001b[1;32m---> 13\u001b[1;33m             train_set, algorithm='megam', trace=0)\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, train_toks, algorithm, trace, encoding, labels, gaussian_prior_sigma, **cutoffs)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'megam'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             return train_maxent_classifier_with_megam(\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[0mtrain_toks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgaussian_prior_sigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcutoffs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             )\n\u001b[0;32m    343\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tadm'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mtrain_maxent_classifier_with_megam\u001b[1;34m(train_toks, trace, encoding, labels, gaussian_prior_sigma, **kwargs)\u001b[0m\n\u001b[0;32m   1487\u001b[0m         \u001b[0moptions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'-multilabel'\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# each possible la\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'multiclass'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainfile_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1489\u001b[1;33m     \u001b[0mstdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_megam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1490\u001b[0m     \u001b[1;31m# print './megam_i686.opt ', ' '.join(options)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m     \u001b[1;31m# Delete the training file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\megam.py\u001b[0m in \u001b[0;36mcall_megam\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'args should be a list of strings'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_megam_bin\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mconfig_megam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;31m# Call megam via a subprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\megam.py\u001b[0m in \u001b[0;36mconfig_megam\u001b[1;34m(bin)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'MEGAM'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mbinary_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'megam.opt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'megam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'megam_686'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'megam_i686.opt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'http://users.umiacs.umd.edu/~hal/megam/index.html'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     )\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    694\u001b[0m     return next(\n\u001b[0;32m    695\u001b[0m         find_binary_iter(\n\u001b[1;32m--> 696\u001b[1;33m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m         )\n\u001b[0;32m    698\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \"\"\"\n\u001b[0;32m    679\u001b[0m     for file in find_file_iter(\n\u001b[1;32m--> 680\u001b[1;33m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m     ):\n\u001b[0;32m    682\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'\\n\\n  For more information on %s, see:\\n    <%s>'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://users.umiacs.umd.edu/~hal/megam/index.html>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\" : pos}\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Windows: I no longer have access to a Windows machine...sorry, download the source.\n",
    "\n",
    "says the hompage..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), \\\n",
    "            (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"), (\"saw\", \"VBD\"), \n",
    "            (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"), (\"on\", \"IN\"), \n",
    "            (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> This parser misses VP for \"saw\" and \"thinks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (CLAUSE\n",
      "    (NP John/NNP)\n",
      "    (VP\n",
      "      thinks/VBZ\n",
      "      (CLAUSE\n",
      "        (NP Mary/NN)\n",
      "        (VP\n",
      "          saw/VBD\n",
      "          (CLAUSE\n",
      "            (NP the/DT cat/NN)\n",
      "            (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))))\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(grammar, loop=3)\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP Alice)\n"
     ]
    }
   ],
   "source": [
    "tree1 = nltk.Tree(\"NP\", [\"Alice\"])\n",
    "print(tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP The rabbit)\n"
     ]
    }
   ],
   "source": [
    "tree2 = nltk.Tree(\"NP\", [\"The\", \"rabbit\"])\n",
    "print(tree2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP chased (NP The rabbit)))\n"
     ]
    }
   ],
   "source": [
    "tree3 = nltk.Tree('VP', ['chased', tree2])\n",
    "tree4 = nltk.Tree('S', [tree1, tree3])\n",
    "print(tree4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP The rabbit)\n"
     ]
    }
   ],
   "source": [
    "print(tree4[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree4.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(t):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        print(t, end=\" \")\n",
    "    else:\n",
    "        print(\"(\", t.label(), end=\" \")\n",
    "        for child in t:\n",
    "            traverse(child)\n",
    "        print(\")\", end=\" \")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (GPE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  few/JJ\n",
      "  industrialized/VBN\n",
      "  nations/NNS\n",
      "  that/WDT\n",
      "  *T*-7/-NONE-\n",
      "  does/VBZ\n",
      "  n't/RB\n",
      "  have/VB\n",
      "  a/DT\n",
      "  higher/JJR\n",
      "  standard/NN\n",
      "  of/IN\n",
      "  regulation/NN\n",
      "  for/IN\n",
      "  the/DT\n",
      "  smooth/JJ\n",
      "  ,/,\n",
      "  needle-like/JJ\n",
      "  fibers/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  crocidolite/NN\n",
      "  that/WDT\n",
      "  *T*-1/-NONE-\n",
      "  are/VBP\n",
      "  classified/VBN\n",
      "  *-5/-NONE-\n",
      "  as/IN\n",
      "  amphobiles/NNS\n",
      "  ,/,\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
      "  ,/,\n",
      "  a/DT\n",
      "  professor/NN\n",
      "  of/IN\n",
      "  pathlogy/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION University/NNP)\n",
      "  of/IN\n",
      "  (PERSON Vermont/NNP College/NNP)\n",
      "  of/IN\n",
      "  (GPE Medicine/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "print(nltk.ne_chunk(sent, binary=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN = re.compile(r\".*\\bin\\b(?!\\b.+ing\\b)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "for doc in nltk.corpus.ieer.parsed_docs(\"NYT_19980315\"):\n",
    "    for rel in nltk.sem.extract_rels(\"ORG\", \"LOC\", doc, corpus=\"ieer\", pattern=IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAN('marco_pantani', 'mercatone_uno')\n",
      "VAN('larmuseau', 'abc_containerline')\n",
      "VAN('horst_köhler', 'imf')\n",
      "VAN('simonet', 'binnenlandse_zaken')\n",
      "VAN('guy_quaden', 'nationale_bank')\n",
      "VAN('de_bauw', 'buitenlandse_zaken')\n",
      "VAN(\"cornet_d'elzius\", 'buitenlandse_handel')\n",
      "VAN('rosenfeld', 'abc_containerline')\n",
      "VAN('carlo_gepts', 'vt4')\n",
      "VAN('lone_leth_larsen', 'deens_cultureel_centrum')\n",
      "VAN('johan_rottiers', 'kardinaal_van_roey_instituut')\n",
      "VAN('jean-louis_peninou', 'international_boundaries_research')\n",
      "VAN('lieven', 'honda')\n",
      "VAN('talal_g_shamoon', 'intertrust_technologies_corporation')\n",
      "VAN('albert_frère', 'tractebel')\n",
      "VAN('robert_spatz', 'okc-beweging')\n",
      "VAN('bart_bode', 'broederlijk_delen')\n",
      "VAN('guido_westerwelle', 'fdp')\n",
      "VAN('martin_bril', 'vrij_nederland')\n",
      "VAN('frank_rijkaard', 'vrij_nederland')\n",
      "VAN('filip', 'telecommunicatie')\n",
      "VAN('maurice_buckmaster', 'special_operations_executive')\n",
      "VAN('mukamba', 'commissie-lumumba')\n",
      "VAN('versnick', 'buitenlandse_zaken')\n",
      "VAN('mukamba', 'miba')\n",
      "VAN('bart_bode', 'broederlijk_delen')\n",
      "VAN('annie_lennox', 'eurythmics')\n",
      "VAN('verheyen', 'duivels')\n",
      "VAN('paul_allen', 'microsoft')\n",
      "VAN('cathy_horyn', 'the_new_york_times')\n",
      "VAN('graydon_carter', 'vanity_fair')\n",
      "VAN('danielle_crittenden', \"women's_quarterly\")\n"
     ]
    }
   ],
   "source": [
    "vnv = \"\"\"\n",
    "(\n",
    "is/V|\n",
    "was/V|\n",
    "werd/V|\n",
    "wordt/V|\n",
    ")\n",
    ".*\n",
    "van/Prep\n",
    "\"\"\"\n",
    "VAN = re.compile(vnv, re.VERBOSE)\n",
    "for doc in conll2002.chunked_sents(\"ned.train\"):\n",
    "    for r in nltk.sem.extract_rels(\"PER\", \"ORG\", doc, corpus=\"conll2002\", pattern=VAN):\n",
    "        print(nltk.sem.clause(r, relsym=\"VAN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...'De/Art ploegmaat/N van/Prep')[PER: 'Marco/N Pantani/N'] 'en/Conj kopman/N van/Prep' [ORG: 'Mercatone/N Uno/N']('in/Prep deze/Pron'...\n",
      "...'In/Prep dezelfde/Pron periode/N was/V')[PER: 'Larmuseau/N'] 'ook/Adv lid/N van/Prep de/Art interkabinettengroep/N rond/Prep' [ORG: 'ABC/N Containerline/N'](',/Punc die/Pron specifiek/Adj was/V opgericht/V'...\n",
      "...'Dit/Pron heeft/V')[PER: 'Horst/N Köhler/Conj'] ',/Punc de/Art in/Prep mei/N aangetreden/V topman/N van/Prep het/Art' [ORG: 'IMF/N'](',/Punc gisteren/Adv gezegd/V in/Prep'...\n",
      "...'')[PER: 'Simonet/N'] 'heeft/V de/Art bekommernissen/N overgemaakt/V aan/Prep minister/N van/Prep' [ORG: 'Binnenlandse/N Zaken/N'](''...\n",
      "...'De/Art woorden/N komen/V van/Prep gouverneur/N')[PER: 'Guy/N Quaden/N'] 'van/Prep de/Art' [ORG: 'Nationale/Adj Bank/N'](',/Punc en/Conj ze/Pron werden/V woensdag/N'...\n",
      "...'')[PER: 'De/Art Bauw/N'] '(/Punc 34/Num )/Punc was/V vroeger/Adj adjunct-woordvoerder/N van/Prep het/Art ministerie/N van/Prep' [ORG: 'Buitenlandse/N Zaken/N']('en/Conj is/V momenteel/Adj adviseur/N van/Prep'...\n",
      "...'')[PER: \"Cornet/V d'Elzius/N\"] 'is/V op/Prep dit/Pron ogenblik/N kabinetsadviseur/N van/Prep staatssecretaris/N voor/Prep' [ORG: 'Buitenlandse/N Handel/N'](''...\n",
      "...'doorspelen/V van/Prep vertrouwelijke/Adj documenten/N aan/Prep')[PER: 'Rosenfeld/N'] ',/Punc zoals/Conj verslagen/V van/Prep de/Art interkabinettenwerkgroep/N rond/Prep' [ORG: 'ABC/N Containerline/N'](',/Punc beschouwde/V'...\n",
      "...'')[PER: 'Carlo/N Gepts/N'] 'over/Prep de/Art radio-/N en/Conj televisieambities/N van/Prep' [ORG: 'VT4/Num']('in/Prep'...\n",
      "...'Een/Art gesprek/N met/Prep')[PER: 'Lone/N Leth/N Larsen/N'] ',/Punc directeur/N van/Prep het/Art' [ORG: 'Deens/Adj Cultureel/Adj Centrum/N']('in/Prep'...\n",
      "...'')[PER: 'Johan/N Rottiers/N'] 'is/V informaticacoördinator/N van/Prep het/Art' [ORG: 'Kardinaal/N Van/N Roey/N Instituut/N']('in/Prep'...\n",
      "...'')[PER: 'Jean-Louis/N Peninou/N'] 'van/Prep de/Art' [ORG: 'International/N Boundaries/N Research/N']('Unit/N van/Prep de/Art universteit/N van/Prep'...\n",
      "...'Een/Art collega/N en/Conj')[PER: 'Lieven/N'] 'van/Prep' [ORG: 'Honda/N']('rijden/V met/Prep de/Art'...\n",
      "...'')[PER: 'Talal/N G./N Shamoon/N'] 'van/Prep' [ORG: 'InterTrust/N Technologies/N Corporation/N'](',/Punc een/Art bedrijf/N uit/Prep'...\n",
      "...'ageert/V sinds/Prep')[PER: 'Albert/N Frère/N'] 'in/Prep 1996/Num zijn/Pron participatie/N van/Prep 24,5/Num procent/N in/Prep' [ORG: 'Tractebel/N']('aan/Prep de/Art'...\n",
      "...'Maar/Conj')[PER: 'Robert/N Spatz/N'] ',/Punc het/Art hoofd/N van/Prep de/Art' [ORG: 'OKC-beweging/N']('in/Prep'...\n",
      "...'')[PER: 'Bart/N Bode/N'] 'van/Prep' [ORG: 'Broederlijk/N Delen/N']('en/Conj economieprofessor/N en/Conj'...\n",
      "...'Secretaris-generaal/N')[PER: 'Guido/N Westerwelle/N'] 'van/Prep de/Art liberale/Adj' [ORG: 'FDP/N'](',/Punc die/Pron jarenlang/Adj met/Prep'...\n",
      "...'dat/Conj een/Art verloren/V stukje/N van/Prep')[PER: 'Martin/N Bril/N'] 'in/Prep datzelfde/Pron nummer/N van/Prep' [ORG: 'Vrij/N Nederland/N'](',/Punc waarin/Adv hij/Pron zijn/Pron passie/N'...\n",
      "...'\"/Punc Het/N geheim/N van/N')[PER: 'Frank/N Rijkaard/N'] '\"/Punc ,/Punc prijkt/V deze/Pron week/N op/Prep de/Art cover/N van/Prep' [ORG: 'Vrij/N Nederland/N'](',/Punc en/Conj daarmee/Adv is/V de/Art'...\n",
      "...'samen/Adv met/Prep prins/N')[PER: 'Filip/N'] 'en/Conj minister/N van/Prep' [ORG: 'Telecommunicatie/N'](''...\n",
      "...'was/V de/Art rechterhand/N van/Prep kolonel/N')[PER: 'Maurice/N Buckmaster/N'] ',/Punc directeur/N van/Prep de/Art' [ORG: 'Special/N Operations/N Executive/N'](',/Punc die/Pron naar/Prep verluidt/V ook/Adv'...\n",
      "...'Omdat/Conj')[PER: 'Mukamba/N'] 'een/Art mogelijke/Adj getuige/N is/V van/Prep de/Art' [ORG: 'commissie-Lumumba/N']('--/N hij/Pron zat/V in/Prep januari/N'...\n",
      "...'werd/V vermoord/V --/N ,/Punc vroeg/V')[PER: 'Versnick/N'] 'als/Conj commissievoorzitter/N aan/Prep minister/N van/Prep' [ORG: 'Buitenlandse/N Zaken/N'](''...\n",
      "...'')[PER: 'Mukamba/N'] 'zou/V zijn/Pron invloed/N als/Conj ex-voorzitter/N van/Prep de/Art' [ORG: 'MIBA/N'](',/Punc het/Art'...\n",
      "...'Deze/Pron week/N praten/V')[PER: 'Bart/N Bode/N'] 'van/Prep' [ORG: 'Broederlijk/N Delen/N']('en/Conj economieprofessor/N en/Conj'...\n",
      "...'Door/Prep rugproblemen/N van/Prep zangeres/N')[PER: 'Annie/N Lennox/N'] 'wordt/V het/Art concert/N van/Prep' [ORG: 'Eurythmics/N']('vandaag/Adv in/Prep'...\n",
      "...'')[PER: 'Verheyen/N'] 'vervoegde/V de/Art rangen/N van/Prep de/Art' [ORG: 'Duivels/N']('in/Prep de/Art tweede/Num wedstrijd/N van/Prep'...\n",
      "...'ontsproten/N aan/Prep het/Art brein/N van/Prep')[PER: 'Paul/N Allen/Pron'] ',/Punc mede-oprichter/N van/Prep' [ORG: 'Microsoft/N'](',/Punc kan/V het/Pron niet/Adv anders/Adv'...\n",
      "...'probeerde/V te/Prep maken/V op/Prep')[PER: 'Cathy/N Horyn/N'] 'van/Prep' [ORG: 'The/N New/N York/N Times/N'](',/Punc toen/Conj hij/Pron haar/Pron ontving/V'...\n",
      "...'--/N hoofdredacteur/N')[PER: 'Graydon/N Carter/N'] 'van/Prep' [ORG: 'Vanity/N Fair/N'](',/Punc hoofdredacteur/N'...\n",
      "...'Zo/Adv leren/Adj ons/Pron de/Art dames/N')[PER: 'Danielle/N Crittenden/N'] ',/Punc miljonairsvrouw/N en/Conj redactrice/N van/Prep de/Art neoconservatieve/Adj' [ORG: \"Women's/N Quarterly/N\"](',/Punc'...\n"
     ]
    }
   ],
   "source": [
    "for doc in conll2002.chunked_sents(\"ned.train\"):\n",
    "    for rel in nltk.sem.extract_rels(\"PER\", \"ORG\", doc, corpus=\"conll2002\", pattern=VAN):\n",
    "        print(nltk.sem.rtuple(rel, lcon=True, rcon=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
