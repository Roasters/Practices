{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2\n",
    "\n",
    "#### Machine Learning in Korea University\n",
    "#### COSE362, Fall 2018\n",
    "#### Due : 11/26 (TUE) 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this assignment, you will learn various classification methods with given datasets.\n",
    "* Implementation detail: Anaconda 5.3 with python 3.7\n",
    "* Use given dataset. Please do not change train / valid / test split.\n",
    "* Use numpy, scikit-learn, and matplotlib library\n",
    "* You don't have to use all imported packages below. (some are optional). <br>\n",
    "Also, you can import additional packages in \"(Option) Other Classifiers\" part. \n",
    "* <b>*DO NOT MODIFY OTHER PARTS OF CODES EXCEPT \"Your Code Here\"*</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Additional packages\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "> 1. Load \"train.csv\". It includes all samples' features and labels.\n",
    "> 2. Training four types of classifiers(logistic regression, decision tree, random forest, support vector machine) and <b>validate</b> it in your own way. <b>(You can't get full credit if you don't conduct validation)</b>\n",
    "> 3. Optionally, if you would train your own classifier(e.g. ensembling or gradient boosting), you can evaluate your own model on the development data. <br>\n",
    "> 4. <b>You should submit your predicted results on test data with the selected classifier in your own manner.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task & dataset description\n",
    "1. 6 Features (1~6)<br>\n",
    "Feature 2, 4, 6 : Real-valued<br>\n",
    "Feature 1, 3, 5 : Categorical <br>\n",
    "\n",
    "2. Samples <br>\n",
    ">In development set : 2,000 samples <br>\n",
    ">In test set : 1,500 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load development dataset\n",
    "Load your development dataset. You should read <b>\"train.csv\"</b>. This is a classification task, and you need to preprocess your data for training your model. <br>\n",
    "> You need to use <b>1-of-K coding scheme</b>, to convert categorical features to one-hot vector. <br>\n",
    "> For example, if there are 3 categorical values, you can convert these features as [1,0,0], [0,1,0], [0,0,1] by 1-of-K coding scheme. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training your model, you need to convert categorical features to one-hot encoding vectors.\n",
    "# Your Code Here\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('./data/train.csv')\n",
    "X_data, y_train = np.array(train_data.iloc[:,:-1]), np.array(train_data.iloc[:,-1])\n",
    "\n",
    "le = LabelEncoder()\n",
    "X_data[:,0], X_data[:,2], X_data[:,4] = map(le.fit_transform, [X_data[:,0], X_data[:,2], X_data[:,4]])\n",
    "\n",
    "numeric_features = [1, 3, 5]\n",
    "\n",
    "# Z-normalization for numerical features\n",
    "for feature_num in numeric_features:\n",
    "    feature_values = X_data[:, feature_num]\n",
    "    feature_mean = np.mean(feature_values)\n",
    "    feature_std = np.std(feature_values)\n",
    "    \n",
    "    X_data[:, feature_num] = (X_data[:, feature_num] - feature_mean) / feature_std\n",
    "    \n",
    "X_train = X_data.copy()\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "categorical_features = [0, 2, 4]\n",
    "\n",
    "for feature_num in categorical_features:\n",
    "    feature_values_tr = X_train[:, feature_num]\n",
    "    feature_set = set(feature_values_tr)\n",
    "    \n",
    "    feature_dict = {}\n",
    "    for i, value in enumerate(feature_set):\n",
    "        feature_dict[value] = i\n",
    "    \n",
    "    for i, value in enumerate(feature_values_tr):\n",
    "        feature_values_tr[i] = feature_dict[value]\n",
    "\n",
    "    one_hot_matriX_train = np.eye(len(feature_set))[feature_values_tr.astype(int)]\n",
    "\n",
    "    X_train = np.concatenate((X_train, one_hot_matriX_train), axis=1)\n",
    "\n",
    "X_train = np.delete(X_train, categorical_features, 1)\n",
    "\n",
    "X_train_ohv = X_train.copy()\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Train and validate your <b>logistic regression classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For choosing the optimal regularization parameter\n",
      "\n",
      "Regularization parameter:  100.0\n",
      "F1 Score:  0.18018405350874023\n",
      "==============================\n",
      "Regularization parameter:  20.0\n",
      "F1 Score:  0.18677884488976837\n",
      "==============================\n",
      "Regularization parameter:  10.0\n",
      "F1 Score:  0.19584122916108573\n",
      "==============================\n",
      "Regularization parameter:  2.0\n",
      "F1 Score:  0.2301127012471597\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter:  1.0\n",
      "F1 Score:  0.2468880340300123\n",
      "==============================\n",
      "Regularization parameter:  0.1\n",
      "F1 Score:  0.28165619208649806\n",
      "==============================\n",
      "Regularization parameter:  0.01\n",
      "F1 Score:  0.27319212346618726\n",
      "==============================\n",
      "Optimal: 0.1, F1 score: 0.28165619208649806\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For choosing the optimal solver\n",
      "\n",
      "Solver:  newton-cg\n",
      "F1 score: 0.283502132638321\n",
      "==============================\n",
      "Solver:  lbfgs\n",
      "F1 score: 0.28165619208649806\n",
      "==============================\n",
      "Solver:  sag\n",
      "F1 score: 0.283502132638321\n",
      "==============================\n",
      "Solver:  saga\n",
      "F1 score: 0.283502132638321\n",
      "==============================\n",
      "Optimal: newton-cg, F1 score: 0.283502132638321\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Optimal regularization parameter: 0.1\n",
      "Optimal solver: newton-cg\n",
      "3-fold cross validaion score 0.290478\n"
     ]
    }
   ],
   "source": [
    "# Training your logistic regression classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# For the case of CV\n",
    "valid_split = 1/5\n",
    "cv = ShuffleSplit(n_splits=5, test_size=valid_split, random_state=0)\n",
    "\n",
    "# Train the model with regularization\n",
    "coefs = [0.01, 0.05, 0.1, 0.5, 1, 10, 100]\n",
    "\n",
    "max_f1 = 0\n",
    "f1 = []\n",
    "\n",
    "print(\"For choosing the optimal regularization parameter\\n\")\n",
    "for coef in coefs:\n",
    "    logreg = LogisticRegression(C=coef, solver='lbfgs', multi_class='multinomial', max_iter=500)\n",
    "    f1_cv = []\n",
    "    for t_index, v_index in cv.split(X_train):\n",
    "        X_tr, X_val = X_train[t_index], X_train[v_index]\n",
    "        y_tr, y_val = y_train[t_index], y_train[v_index]\n",
    "        logreg.fit(X_tr, y_tr)\n",
    "        y_pred = logreg.predict(X_val)\n",
    "        f1_cv.append(f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "    mean_f1 = np.mean(f1_cv)\n",
    "    f1.append(mean_f1)\n",
    "    if max_f1 < mean_f1:\n",
    "        max_f1 = mean_f1\n",
    "        opt_coef = coef\n",
    "\n",
    "    print(\"Regularization parameter: \", 1/coef)\n",
    "    print(\"F1 Score: \", mean_f1)\n",
    "    print(\"=\"*30)\n",
    "\n",
    "print(\"Optimal: {}, F1 score: {}\".format(1/opt_coef, max_f1))\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "max_f1 = 0\n",
    "solvers = ['newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "\n",
    "print(\"For choosing the optimal solver\\n\")\n",
    "for solver in solvers:\n",
    "    logreg = LogisticRegression(C=opt_coef, solver=solver, multi_class='multinomial', max_iter=1800)\n",
    "    f1_cv = []\n",
    "    for t_index, v_index in cv.split(X_train):\n",
    "        X_tr, X_val = X_train[t_index], X_train[v_index]\n",
    "        y_tr, y_val = y_train[t_index], y_train[v_index]\n",
    "        logreg.fit(X_tr, y_tr)\n",
    "        y_pred = logreg.predict(X_val)\n",
    "        f1_cv.append(f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "    mean_f1 = np.mean(f1_cv)\n",
    "    if max_f1 < mean_f1:\n",
    "        max_f1 = mean_f1\n",
    "        opt_sol = solver\n",
    "    print(\"Solver: \", solver)\n",
    "    print(\"F1 score:\", mean_f1)\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "print(\"Optimal: {}, F1 score: {}\".format(opt_sol, max_f1))\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(C=opt_coef, solver=opt_sol, max_iter=1800, multi_class='multinomial')\n",
    "\n",
    "print(\"Optimal regularization parameter: {}\\nOptimal solver: {}\\n3-fold cross validaion score {:.6f}\".\\\n",
    "     format(1/opt_coef, opt_sol, np.mean(cross_val_score(logreg, X_train, y_train, cv=3))))\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Train and validate your <b>decision tree classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "# of selected feature(s) : 1\n",
      "Selected feature of this iteration : 4\n",
      "F1 score: 0.28491780\n",
      "==================================================\n",
      "# of selected feature(s) : 2\n",
      "Selected feature of this iteration : 1\n",
      "F1 score: 0.20820198\n",
      "==================================================\n",
      "# of selected feature(s) : 3\n",
      "Selected feature of this iteration : 5\n",
      "F1 score: 0.24934150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "# of selected feature(s) : 4\n",
      "Selected feature of this iteration : 0\n",
      "F1 score: 0.31586097\n",
      "==================================================\n",
      "# of selected feature(s) : 5\n",
      "Selected feature of this iteration : 3\n",
      "F1 score: 0.35752144\n",
      "==================================================\n",
      "# of selected feature(s) : 6\n",
      "Selected feature of this iteration : 2\n",
      "F1 score: 0.38406268\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Selected features:  [4, 1, 5, 0, 3, 2]\n",
      "3-fold cross validation score of this model:  0.4354333529839798\n"
     ]
    }
   ],
   "source": [
    "# Training your decision tree classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "X_train = X_data.copy()\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "\n",
    "# Feature selection\n",
    "sel_num = X_train.shape[1]\n",
    "selected_feature = []\n",
    "selected_f1 = []\n",
    "\n",
    "for sel in range(sel_num) :\n",
    "    max_f1 = 0\n",
    "    min_feature = 0\n",
    "    \n",
    "    # For each feature\n",
    "    for i in range(X_train.shape[1]) :\n",
    "        f1_ith = []\n",
    "        \n",
    "        if i in selected_feature:\n",
    "            continue\n",
    "        X_tr = X_train[:, selected_feature + [i]]\n",
    "        \n",
    "        # For cross validation\n",
    "        for train_index, val_index in cv.split(X_train) :\n",
    "            X_tr_cv, X_val = X_tr[train_index], X_tr[val_index]\n",
    "            y_tr_cv, y_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "            # Derive f1 score\n",
    "            dt.fit(X_tr_cv, y_tr_cv)\n",
    "            y_pred = dt.predict(X_val)\n",
    "            f1_ith.append(f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "\n",
    "        if np.mean(f1_ith) > max_f1:\n",
    "            max_f1 = np.mean(f1_ith)\n",
    "            min_feature = i\n",
    "            opt_params = dt.get_params()\n",
    "\n",
    "    print('='*50)\n",
    "    print(\"# of selected feature(s) : {}\".format(sel+1))\n",
    "    print(\"Selected feature of this iteration : {}\".format(min_feature))\n",
    "    print(\"F1 score: {:.8f}\".format(max_f1))\n",
    "    selected_feature.append(min_feature)\n",
    "    selected_f1.append(max_f1)\n",
    "\n",
    "print(\"\\n\\n\\n\")    \n",
    "\n",
    "selected_feature = selected_feature[:selected_f1.index(max(selected_f1))+1]\n",
    "X_train = X_train[:, selected_feature]\n",
    "\n",
    "print(\"Selected features: \", selected_feature)\n",
    "print(\"3-fold cross validation score of this model: \", np.mean(cross_val_score(dt, X_train, y_train, cv = 3)))\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Train and validate your <b>random forest classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  5\n",
      "F1 score:  0.3684475152545085\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  10\n",
      "F1 score:  0.41688447130181555\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  15\n",
      "F1 score:  0.4269743546613952\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  20\n",
      "F1 score:  0.41748892136043125\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  25\n",
      "F1 score:  0.436561404271713\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  30\n",
      "F1 score:  0.44789144035029305\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  35\n",
      "F1 score:  0.44966763296766327\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  40\n",
      "F1 score:  0.45877048961196454\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  45\n",
      "F1 score:  0.45589503520366714\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  50\n",
      "F1 score:  0.45971643069335333\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Optimal number of trees:  50\n",
      "3-fold cross validation score of this model:  0.4774643516205715\n"
     ]
    }
   ],
   "source": [
    "# Training your random forest classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "X_train = X_data.copy()\n",
    "\n",
    "num_set = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]   \n",
    "\n",
    "max_f1 = 0\n",
    "\n",
    "for n in num_set:\n",
    "    f1_cv = []\n",
    "    rfc = RandomForestClassifier(n_estimators=n ,criterion=\"entropy\", random_state=0)\n",
    "    for t_index, v_index in cv.split(X_train):\n",
    "        X_tr, y_tr = X_train[t_index], y_train[t_index]\n",
    "        X_val, y_val = X_train[v_index], y_train[v_index]\n",
    "        rfc.fit(X_tr, y_tr)\n",
    "        y_pred = rfc.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "        f1_cv.append(f1)\n",
    "    if max_f1 < np.mean(f1_cv):\n",
    "        max_f1 = np.mean(f1_cv)\n",
    "        opt_n = n\n",
    "    print(\"The number of trees: \", n)\n",
    "    print(\"F1 score: \", np.mean(f1_cv))\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Optimal number of trees: \", opt_n)\n",
    "print(\"3-fold cross validation score of this model: \", np.mean(cross_val_score(\\\n",
    "                        RandomForestClassifier(n_estimators=opt_n, criterion='entropy'), X_train, y_train, cv = 3)))\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "Train and validate your <b>support vector machine classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For choosing the optimal regularization parameter\n",
      "\n",
      "Regularization parameter:  0.001\n",
      "F1 score:  0.294155912528284\n",
      "========================================\n",
      "Regularization parameter:  0.01\n",
      "F1 score:  0.294155912528284\n",
      "========================================\n",
      "Regularization parameter:  0.05\n",
      "F1 score:  0.15677424975336465\n",
      "========================================\n",
      "Regularization parameter:  0.1\n",
      "F1 score:  0.15637113397087893\n",
      "========================================\n",
      "Regularization parameter:  1.0\n",
      "F1 score:  0.23887572379076266\n",
      "========================================\n",
      "Regularization parameter:  5.0\n",
      "F1 score:  0.35681018433278033\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter:  10.0\n",
      "F1 score:  0.40038368405115704\n",
      "========================================\n",
      "Regularization parameter:  50.0\n",
      "F1 score:  0.42141988168121775\n",
      "========================================\n",
      "Regularization parameter:  100.0\n",
      "F1 score:  0.4232442372826927\n",
      "========================================\n",
      "Regularization parameter:  1000.0\n",
      "F1 score:  0.4046013696307599\n",
      "========================================\n",
      "Optimal regularization parameter:  100.0\n",
      "F1 score: 0.423244\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For choosing the optimal kernel function\n",
      "\n",
      "Current kernel:  poly\n",
      "F1 score:  0.40958825673060983\n",
      "========================================\n",
      "Current kernel:  rbf\n",
      "F1 score:  0.4232442372826927\n",
      "========================================\n",
      "Current kernel:  sigmoid\n",
      "F1 score:  0.26266681223909444\n",
      "========================================\n",
      "Optimal kerenel function:  rbf\n",
      "F1 score: 0.423244\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Optimal regularization parameter:  100.0\n",
      "Optimal kernel:  rbf\n",
      "3-fold cross validation score: 0.465432\n"
     ]
    }
   ],
   "source": [
    "# Training your support vector machine classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "X_train = X_train_ohv.copy()\n",
    "\n",
    "reg_params = [0.001, 0.01, 0.05, 0.1, 1.0, 5.0, 10.0, 50.0, 100.0, 1000.0]\n",
    "max_f1 = 0\n",
    "\n",
    "print(\"For choosing the optimal regularization parameter\\n\")\n",
    "for param in reg_params:\n",
    "    f1_cv = []\n",
    "    svc = SVC(C=param)\n",
    "    for t_index, v_index in cv.split(X_train):\n",
    "        X_tr, y_tr = X_train[t_index], y_train[t_index]\n",
    "        X_val, y_val = X_train[v_index], y_train[v_index]\n",
    "        svc.fit(X_tr, y_tr)\n",
    "        y_pred = svc.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "        f1_cv.append(f1)\n",
    "    if max_f1 < np.mean(f1_cv):\n",
    "        max_f1 = np.mean(f1_cv)\n",
    "        opt_param = param\n",
    "    print(\"Regularization parameter: \", param)\n",
    "    print(\"F1 score: \", np.mean(f1_cv))\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"Optimal regularization parameter: \", opt_param)\n",
    "print(\"F1 score: {:.6f}\".format(max_f1))\n",
    "print(\"\\n\\n\\n\")    \n",
    "    \n",
    "kernels = ['poly', 'rbf', 'sigmoid']\n",
    "max_f1 = 0\n",
    "\n",
    "print(\"For choosing the optimal kernel function\\n\")\n",
    "for kernel in kernels:\n",
    "    f1_cv = []\n",
    "    svc = SVC(C=opt_param, kernel=kernel)\n",
    "    for t_index, v_index in cv.split(X_train):\n",
    "        X_tr, y_tr = X_train[t_index], y_train[t_index]\n",
    "        X_val, y_val = X_train[v_index], y_train[v_index]\n",
    "        svc.fit(X_tr, y_tr)\n",
    "        y_pred = svc.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "        f1_cv.append(f1)\n",
    "    if max_f1 < np.mean(f1_cv):\n",
    "        max_f1 = np.mean(f1_cv)\n",
    "        opt_kernel = kernel\n",
    "    print(\"Current kernel: \", kernel)\n",
    "    print(\"F1 score: \", np.mean(f1_cv))\n",
    "    print(\"=\"*40)\n",
    "print(\"Optimal kerenel function: \", opt_kernel)\n",
    "print(\"F1 score: {:.6f}\".format(max_f1))\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "print(\"Optimal regularization parameter: \", opt_param)    \n",
    "print(\"Optimal kernel: \", opt_kernel)   \n",
    "print(\"3-fold cross validation score: {:.6f}\".format(cross_val_score(\\\n",
    "                            SVC(C=opt_param, kernel=opt_kernel), X_train, y_train, cv=3).mean()))\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Option) Other Classifiers.\n",
    "Train and validate other classifiers by your own manner.\n",
    "> <b> If you need, you can import other models only in this cell, only in scikit-learn. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current model for bagging:  Logistic Regression \n",
      "\n",
      "With 5 models, F1 score is 0.194204\n",
      "Accuracy: 0.257000\n",
      "========================================\n",
      "With 10 models, F1 score is 0.187815\n",
      "Accuracy: 0.252000\n",
      "========================================\n",
      "With 15 models, F1 score is 0.195679\n",
      "Accuracy: 0.263000\n",
      "========================================\n",
      "With 20 models, F1 score is 0.186904\n",
      "Accuracy: 0.253500\n",
      "========================================\n",
      "With 25 models, F1 score is 0.193956\n",
      "Accuracy: 0.257500\n",
      "========================================\n",
      "With 30 models, F1 score is 0.190547\n",
      "Accuracy: 0.257500\n",
      "========================================\n",
      "With 35 models, F1 score is 0.189012\n",
      "Accuracy: 0.256000\n",
      "========================================\n",
      "With 40 models, F1 score is 0.190781\n",
      "Accuracy: 0.260000\n",
      "========================================\n",
      "Optimal number of models:  15\n",
      "3-fold cross validation score: 0.274439\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current model for bagging:  Decision Tree \n",
      "\n",
      "With 5 models, F1 score is 0.429073\n",
      "Accuracy: 0.466000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 10 models, F1 score is 0.452904\n",
      "Accuracy: 0.481500\n",
      "========================================\n",
      "With 15 models, F1 score is 0.468887\n",
      "Accuracy: 0.495000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20 models, F1 score is 0.454635\n",
      "Accuracy: 0.485000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 25 models, F1 score is 0.479146\n",
      "Accuracy: 0.500500\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 30 models, F1 score is 0.477688\n",
      "Accuracy: 0.501500\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 35 models, F1 score is 0.459558\n",
      "Accuracy: 0.499000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 40 models, F1 score is 0.466857\n",
      "Accuracy: 0.504500\n",
      "========================================\n",
      "Optimal number of models:  25\n",
      "3-fold cross validation score: 0.501940\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current model for bagging:  Random Forest \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 5 models, F1 score is 0.433194\n",
      "Accuracy: 0.472000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 10 models, F1 score is 0.462866\n",
      "Accuracy: 0.480000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 15 models, F1 score is 0.445912\n",
      "Accuracy: 0.483000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20 models, F1 score is 0.455900\n",
      "Accuracy: 0.480000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 25 models, F1 score is 0.464734\n",
      "Accuracy: 0.486000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 30 models, F1 score is 0.472419\n",
      "Accuracy: 0.492000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 35 models, F1 score is 0.462846\n",
      "Accuracy: 0.488000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 40 models, F1 score is 0.451946\n",
      "Accuracy: 0.481000\n",
      "========================================\n",
      "Optimal number of models:  30\n",
      "3-fold cross validation score: 0.491945\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current model for bagging:  Support Vector Machine \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 5 models, F1 score is 0.405933\n",
      "Accuracy: 0.444500\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 10 models, F1 score is 0.419313\n",
      "Accuracy: 0.455500\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 15 models, F1 score is 0.408988\n",
      "Accuracy: 0.452500\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20 models, F1 score is 0.433607\n",
      "Accuracy: 0.459500\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 25 models, F1 score is 0.424960\n",
      "Accuracy: 0.460000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 30 models, F1 score is 0.415550\n",
      "Accuracy: 0.462000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 35 models, F1 score is 0.419730\n",
      "Accuracy: 0.456000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 40 models, F1 score is 0.424578\n",
      "Accuracy: 0.466000\n",
      "========================================\n",
      "Optimal number of models:  20\n",
      "3-fold cross validation score: 0.460479\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If you need additional packages, import your own packages below.\n",
    "# Your Code Here\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "models = {\"Logistic Regression\" : LogisticRegression(C=0.1, solver='lbfgs', max_iter=2000, multi_class='multinomial'),\n",
    "         \"Decision Tree\" : DecisionTreeClassifier(criterion=\"entropy\", random_state=0),\n",
    "         \"Random Forest\" : RandomForestClassifier(n_estimators=50, criterion=\"entropy\", random_state=0),\n",
    "         \"Support Vector Machine\" : SVC(C=100.0, kernel=\"rbf\")}\n",
    "num_set = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "\n",
    "for model in models.keys():\n",
    "    print(\"\\nCurrent model for bagging: \", model, \"\\n\")\n",
    "    \n",
    "    max_f1 = 0\n",
    "    \n",
    "    if model == \"Logistic Regression\" or model == \"Support Vector Machine\":\n",
    "        X_train = X_train_ohv\n",
    "    else:\n",
    "        X_train = X_data\n",
    "        \n",
    "    for n in num_set:\n",
    "        f1_cv = []\n",
    "        accuracy_cv = []\n",
    "        bag = BaggingClassifier(base_estimator=models[model], n_estimators=n)\n",
    "        for t_index, v_index in cv.split(X_train):\n",
    "            X_tr, y_tr = X_train[t_index], y_train[t_index]\n",
    "            X_val, y_val = X_train[v_index], y_train[v_index]\n",
    "            \n",
    "            bag.fit(X_tr, y_tr)\n",
    "            y_pred = bag.predict(X_val)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "            f1_cv.append(f1)\n",
    "            \n",
    "            accuracy = len(y_pred[y_pred == y_val]) / len(y_pred)\n",
    "            accuracy_cv.append(accuracy)\n",
    "            \n",
    "        if max_f1 < np.mean(f1_cv):\n",
    "            max_f1 = np.mean(f1_cv)\n",
    "            opt_n = n\n",
    "            \n",
    "        print(\"With {} models, F1 score is {:.6f}\".format(n, np.mean(f1_cv)))\n",
    "        print(\"Accuracy: {:.6f}\".format(np.mean(accuracy_cv)))\n",
    "        print(\"=\"*40)\n",
    "    print(\"Optimal number of models: \", opt_n)\n",
    "    print(\"3-fold cross validation score: {:.6f}\".format(cross_val_score(BaggingClassifier(base_estimator=models[model], n_estimators=opt_n), X_train, y_train, cv=3).mean()))\n",
    "    print(\"\\n\\n\\n\")\n",
    "#End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit your prediction on the test data.\n",
    "\n",
    "* Select your model and explain it briefly.\n",
    "* You should read <b>\"test.csv\"</b>.\n",
    "* Prerdict your model with dictionary form.\n",
    "* Prediction example <br>\n",
    "[2, 6, 14, 8, $\\cdots$]\n",
    "* We will rank your result by <b>F1 metric(with 'macro' option)</b>.\n",
    "* <b> If you don't submit prediction file or submit it in wrong format, you can't get the point for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Explain your final model\n",
    "각각의 모델 별로 여러 parameter를 조정하여 F1 score가 가장 높게 나오는 모델을 찾아낸 뒤, 해당 모델을 ensemble 모델 중 bagging 모델에 적용하였습니다. bagging을 선택한 이유는, target class의 수가 많은 것에 비해 학습하려는 데이터의 숫자가 많지 않다는 판단이 들어서 좀더 stable한 모델을 만들기 위해서입니다. bagging에서는 새로운 데이터 셋의 개수만을 조정하여 가장 적합한 모델을 찾아봤습니다. 그 결과 Decision Tree를 통해 40개의 새로 추출된 데이터셋으로 bagging을 통해 학습했을 때 가장 점수가 높게 나왔고, Decision Tree와 Randoom Forest의 경우 categorical feature를 one-hot encoding을 하지 않았을 때가 encoding을 한 경우 보다 더 높은 점수를 냈기에 one-hot encoding을 하지 않고 진행하였습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset.\n",
    "# Your Code Here\n",
    "X_test = pd.read_csv('./data/test.csv')\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "le = LabelEncoder()\n",
    "X_test[:,0], X_test[:,2], X_test[:,4] = map(le.fit_transform, [X_test[:,0], X_test[:,2], X_test[:,4]])\n",
    "\n",
    "numeric_features = [1, 3, 5]\n",
    "\n",
    "for feature_num in numeric_features:\n",
    "    feature_values = X_test[:, feature_num]\n",
    "    feature_mean = np.mean(feature_values)\n",
    "    feature_std = np.std(feature_values)\n",
    "    \n",
    "    X_test[:, feature_num] = (X_test[:, feature_num] - feature_mean) / feature_std\n",
    "    \n",
    "X_train = X_data.copy()\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target class\n",
    "# Make variable \"my_answer\", type of array, and fill this array with your class predictions.\n",
    "# Modify file name into your student number and your name.\n",
    "# Your Code Here\n",
    "bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy', random_state=0), n_estimators=25)\n",
    "my_answer = bag.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "file_name = \"HW2_2012130730_ShimJaeheon.csv\"\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is for saving predicted answers. DO NOT MODIFY.\n",
    "pd.Series(my_answer).to_csv(\"./data/\" + file_name, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
