{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2\n",
    "\n",
    "#### Machine Learning in Korea University\n",
    "#### COSE362, Fall 2018\n",
    "#### Due : 11/26 (TUE) 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this assignment, you will learn various classification methods with given datasets.\n",
    "* Implementation detail: Anaconda 5.3 with python 3.7\n",
    "* Use given dataset. Please do not change train / valid / test split.\n",
    "* Use numpy, scikit-learn, and matplotlib library\n",
    "* You don't have to use all imported packages below. (some are optional). <br>\n",
    "Also, you can import additional packages in \"(Option) Other Classifiers\" part. \n",
    "* <b>*DO NOT MODIFY OTHER PARTS OF CODES EXCEPT \"Your Code Here\"*</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Additional packages\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "> 1. Load \"train.csv\". It includes all samples' features and labels.\n",
    "> 2. Training four types of classifiers(logistic regression, decision tree, random forest, support vector machine) and <b>validate</b> it in your own way. <b>(You can't get full credit if you don't conduct validation)</b>\n",
    "> 3. Optionally, if you would train your own classifier(e.g. ensembling or gradient boosting), you can evaluate your own model on the development data. <br>\n",
    "> 4. <b>You should submit your predicted results on test data with the selected classifier in your own manner.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task & dataset description\n",
    "1. 6 Features (1~6)<br>\n",
    "Feature 2, 4, 6 : Real-valued<br>\n",
    "Feature 1, 3, 5 : Categorical <br>\n",
    "\n",
    "2. Samples <br>\n",
    ">In development set : 2,000 samples <br>\n",
    ">In test set : 1,500 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load development dataset\n",
    "Load your development dataset. You should read <b>\"train.csv\"</b>. This is a classification task, and you need to preprocess your data for training your model. <br>\n",
    "> You need to use <b>1-of-K coding scheme</b>, to convert categorical features to one-hot vector. <br>\n",
    "> For example, if there are 3 categorical values, you can convert these features as [1,0,0], [0,1,0], [0,0,1] by 1-of-K coding scheme. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training your model, you need to convert categorical features to one-hot encoding vectors.\n",
    "# Your Code Here\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('./data/train.csv')\n",
    "X_data, y_train = np.array(train_data.iloc[:,:-1]), np.array(train_data.iloc[:,-1])\n",
    "\n",
    "le = LabelEncoder()\n",
    "X_data[:,0], X_data[:,2], X_data[:,4] = map(le.fit_transform, [X_data[:,0], X_data[:,2], X_data[:,4]])\n",
    "\n",
    "numeric_features = [1, 3, 5]\n",
    "\n",
    "# Z-normalization for numerical features\n",
    "for feature_num in numeric_features:\n",
    "    feature_values = X_data[:, feature_num]\n",
    "    feature_mean = np.mean(feature_values)\n",
    "    feature_std = np.std(feature_values)\n",
    "    \n",
    "    X_data[:, feature_num] = (X_data[:, feature_num] - feature_mean) / feature_std\n",
    "    \n",
    "X_train = X_data.copy()\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "categorical_features = [0, 2, 4]\n",
    "\n",
    "for feature_num in categorical_features:\n",
    "    feature_values_tr = X_train[:, feature_num]\n",
    "    feature_set = set(feature_values_tr)\n",
    "    \n",
    "    feature_dict = {}\n",
    "    for i, value in enumerate(feature_set):\n",
    "        feature_dict[value] = i\n",
    "    \n",
    "    for i, value in enumerate(feature_values_tr):\n",
    "        feature_values_tr[i] = feature_dict[value]\n",
    "\n",
    "    one_hot_matriX_train = np.eye(len(feature_set))[feature_values_tr.astype(int)]\n",
    "\n",
    "    X_train = np.concatenate((X_train, one_hot_matriX_train), axis=1)\n",
    "\n",
    "X_train = np.delete(X_train, categorical_features, 1)\n",
    "\n",
    "X_train_ohv = X_train.copy()\n",
    "\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Train and validate your <b>logistic regression classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For choosing the optimal regularization parameter\n",
      "\n",
      "Regularization parameter:  100.0\n",
      "F1 Score:  0.18018405350874023\n",
      "==============================\n",
      "Regularization parameter:  20.0\n",
      "F1 Score:  0.18677884488976837\n",
      "==============================\n",
      "Regularization parameter:  10.0\n",
      "F1 Score:  0.19584122916108573\n",
      "==============================\n",
      "Regularization parameter:  2.0\n",
      "F1 Score:  0.2301127012471597\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter:  1.0\n",
      "F1 Score:  0.2468880340300123\n",
      "==============================\n",
      "Regularization parameter:  0.1\n",
      "F1 Score:  0.28165619208649806\n",
      "==============================\n",
      "Regularization parameter:  0.01\n",
      "F1 Score:  0.27319212346618726\n",
      "==============================\n",
      "Optimal: 0.1, F1 score: 0.28165619208649806\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For choosing the optimal solver\n",
      "\n",
      "Solver:  newton-cg\n",
      "F1 score: 0.283502132638321\n",
      "==============================\n",
      "Solver:  lbfgs\n",
      "F1 score: 0.28165619208649806\n",
      "==============================\n",
      "Solver:  sag\n",
      "F1 score: 0.283502132638321\n",
      "==============================\n",
      "Solver:  saga\n",
      "F1 score: 0.283502132638321\n",
      "==============================\n",
      "Optimal: newton-cg, F1 score: 0.283502132638321\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Optimal regularization parameter: 0.1\n",
      "Optimal solver: newton-cg\n",
      "3-fold cross validaion score 0.290478\n"
     ]
    }
   ],
   "source": [
    "# Training your logistic regression classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# For the case of CV\n",
    "valid_split = 1/5\n",
    "cv = ShuffleSplit(n_splits=5, test_size=valid_split, random_state=0)\n",
    "\n",
    "# Train the model with regularization\n",
    "coefs = [0.01, 0.05, 0.1, 0.5, 1, 10, 100]\n",
    "\n",
    "max_f1 = 0\n",
    "f1 = []\n",
    "\n",
    "print(\"For choosing the optimal regularization parameter\\n\")\n",
    "for coef in coefs:\n",
    "    logreg = LogisticRegression(C=coef, solver='lbfgs', multi_class='multinomial', max_iter=500)\n",
    "    f1_cv = []\n",
    "    for t_index, v_index in cv.split(X_train):\n",
    "        X_tr, X_val = X_train[t_index], X_train[v_index]\n",
    "        y_tr, y_val = y_train[t_index], y_train[v_index]\n",
    "        logreg.fit(X_tr, y_tr)\n",
    "        y_pred = logreg.predict(X_val)\n",
    "        f1_cv.append(f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "    mean_f1 = np.mean(f1_cv)\n",
    "    f1.append(mean_f1)\n",
    "    if max_f1 < mean_f1:\n",
    "        max_f1 = mean_f1\n",
    "        opt_coef = coef\n",
    "\n",
    "    print(\"Regularization parameter: \", 1/coef)\n",
    "    print(\"F1 Score: \", mean_f1)\n",
    "    print(\"=\"*30)\n",
    "\n",
    "print(\"Optimal: {}, F1 score: {}\".format(1/opt_coef, max_f1))\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "max_f1 = 0\n",
    "solvers = ['newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "\n",
    "print(\"For choosing the optimal solver\\n\")\n",
    "for solver in solvers:\n",
    "    logreg = LogisticRegression(C=opt_coef, solver=solver, multi_class='multinomial', max_iter=1800)\n",
    "    f1_cv = []\n",
    "    for t_index, v_index in cv.split(X_train):\n",
    "        X_tr, X_val = X_train[t_index], X_train[v_index]\n",
    "        y_tr, y_val = y_train[t_index], y_train[v_index]\n",
    "        logreg.fit(X_tr, y_tr)\n",
    "        y_pred = logreg.predict(X_val)\n",
    "        f1_cv.append(f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "    mean_f1 = np.mean(f1_cv)\n",
    "    if max_f1 < mean_f1:\n",
    "        max_f1 = mean_f1\n",
    "        opt_sol = solver\n",
    "    print(\"Solver: \", solver)\n",
    "    print(\"F1 score:\", mean_f1)\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "print(\"Optimal: {}, F1 score: {}\".format(opt_sol, max_f1))\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(C=opt_coef, solver=opt_sol, max_iter=1800, multi_class='multinomial')\n",
    "\n",
    "print(\"Optimal regularization parameter: {}\\nOptimal solver: {}\\n3-fold cross validaion score {:.6f}\".\\\n",
    "     format(1/opt_coef, opt_sol, np.mean(cross_val_score(logreg, X_train, y_train, cv=3))))\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Train and validate your <b>decision tree classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "# of selected feature(s) : 1\n",
      "Selected feature of this iteration : 4\n",
      "F1 score: 0.28491780\n",
      "==================================================\n",
      "# of selected feature(s) : 2\n",
      "Selected feature of this iteration : 1\n",
      "F1 score: 0.20820198\n",
      "==================================================\n",
      "# of selected feature(s) : 3\n",
      "Selected feature of this iteration : 5\n",
      "F1 score: 0.24934150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "# of selected feature(s) : 4\n",
      "Selected feature of this iteration : 0\n",
      "F1 score: 0.31586097\n",
      "==================================================\n",
      "# of selected feature(s) : 5\n",
      "Selected feature of this iteration : 3\n",
      "F1 score: 0.35752144\n",
      "==================================================\n",
      "# of selected feature(s) : 6\n",
      "Selected feature of this iteration : 2\n",
      "F1 score: 0.38406268\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Selected features:  [4, 1, 5, 0, 3, 2]\n",
      "3-fold cross validation score of this model:  0.4354333529839798\n"
     ]
    }
   ],
   "source": [
    "# Training your decision tree classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "X_train = X_data.copy()\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "\n",
    "# Feature selection\n",
    "sel_num = X_train.shape[1]\n",
    "selected_feature = []\n",
    "selected_f1 = []\n",
    "\n",
    "for sel in range(sel_num) :\n",
    "    max_f1 = 0\n",
    "    min_feature = 0\n",
    "    \n",
    "    # For each feature\n",
    "    for i in range(X_train.shape[1]) :\n",
    "        f1_ith = []\n",
    "        \n",
    "        if i in selected_feature:\n",
    "            continue\n",
    "        X_tr = X_train[:, selected_feature + [i]]\n",
    "        \n",
    "        # For cross validation\n",
    "        for train_index, val_index in cv.split(X_train) :\n",
    "            X_tr_cv, X_val = X_tr[train_index], X_tr[val_index]\n",
    "            y_tr_cv, y_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "            # Derive f1 score\n",
    "            dt.fit(X_tr_cv, y_tr_cv)\n",
    "            y_pred = dt.predict(X_val)\n",
    "            f1_ith.append(f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred)))\n",
    "\n",
    "        if np.mean(f1_ith) > max_f1:\n",
    "            max_f1 = np.mean(f1_ith)\n",
    "            min_feature = i\n",
    "            opt_params = dt.get_params()\n",
    "\n",
    "    print('='*50)\n",
    "    print(\"# of selected feature(s) : {}\".format(sel+1))\n",
    "    print(\"Selected feature of this iteration : {}\".format(min_feature))\n",
    "    print(\"F1 score: {:.8f}\".format(max_f1))\n",
    "    selected_feature.append(min_feature)\n",
    "    selected_f1.append(max_f1)\n",
    "\n",
    "print(\"\\n\\n\\n\")    \n",
    "\n",
    "selected_feature = selected_feature[:selected_f1.index(max(selected_f1))+1]\n",
    "X_train = X_train[:, selected_feature]\n",
    "\n",
    "print(\"Selected features: \", selected_feature)\n",
    "print(\"3-fold cross validation score of this model: \", np.mean(cross_val_score(dt, X_train, y_train, cv = 3)))\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Train and validate your <b>random forest classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  5\n",
      "F1 score:  0.3684475152545085\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  10\n",
      "F1 score:  0.41688447130181555\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  15\n",
      "F1 score:  0.4269743546613952\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  20\n",
      "F1 score:  0.41748892136043125\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  25\n",
      "F1 score:  0.436561404271713\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  30\n",
      "F1 score:  0.44789144035029305\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  35\n",
      "F1 score:  0.44966763296766327\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  40\n",
      "F1 score:  0.45877048961196454\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  45\n",
      "F1 score:  0.45589503520366714\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trees:  50\n",
      "F1 score:  0.45971643069335333\n",
      "========================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Optimal number of trees:  50\n",
      "3-fold cross validation score of this model:  0.4774643516205715\n"
     ]
    }
   ],
   "source": [
    "# Training your random forest classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "X_train = X_data.copy()\n",
    "\n",
    "num_set = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]   \n",
    "\n",
    "max_f1 = 0\n",
    "\n",
    "for n in num_set:\n",
    "    f1_cv = []\n",
    "    rfc = RandomForestClassifier(n_estimators=n ,criterion=\"entropy\", random_state=0)\n",
    "    for t_index, v_index in cv.split(X_train):\n",
    "        X_tr, y_tr = X_train[t_index], y_train[t_index]\n",
    "        X_val, y_val = X_train[v_index], y_train[v_index]\n",
    "        rfc.fit(X_tr, y_tr)\n",
    "        y_pred = rfc.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "        f1_cv.append(f1)\n",
    "    if max_f1 < np.mean(f1_cv):\n",
    "        max_f1 = np.mean(f1_cv)\n",
    "        opt_n = n\n",
    "    print(\"The number of trees: \", n)\n",
    "    print(\"F1 score: \", np.mean(f1_cv))\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Optimal number of trees: \", opt_n)\n",
    "print(\"3-fold cross validation score of this model: \", np.mean(cross_val_score(\\\n",
    "                        RandomForestClassifier(n_estimators=opt_n, criterion='entropy'), X_train, y_train, cv = 3)))\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "Train and validate your <b>support vector machine classifier</b>, and print out your validation(or cross-validation) error.\n",
    "> If you want, you can use cross validation, regularization, or feature selection methods. <br>\n",
    "> <b> You should use F1 score('macro' option) as evaluation metric. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For choosing the optimal regularization parameter\n",
      "\n",
      "Regularization parameter:  0.001\n",
      "F1 score:  0.294155912528284\n",
      "========================================\n",
      "Regularization parameter:  0.01\n",
      "F1 score:  0.294155912528284\n",
      "========================================\n",
      "Regularization parameter:  0.05\n",
      "F1 score:  0.15677424975336465\n",
      "========================================\n",
      "Regularization parameter:  0.1\n",
      "F1 score:  0.15637113397087893\n",
      "========================================\n",
      "Regularization parameter:  1.0\n",
      "F1 score:  0.23887572379076266\n",
      "========================================\n",
      "Regularization parameter:  5.0\n",
      "F1 score:  0.35681018433278033\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter:  10.0\n",
      "F1 score:  0.40038368405115704\n",
      "========================================\n",
      "Regularization parameter:  50.0\n",
      "F1 score:  0.42141988168121775\n",
      "========================================\n",
      "Regularization parameter:  100.0\n",
      "F1 score:  0.4232442372826927\n",
      "========================================\n",
      "Regularization parameter:  1000.0\n",
      "F1 score:  0.4046013696307599\n",
      "========================================\n",
      "Optimal regularization parameter:  100.0\n",
      "F1 score: 0.423244\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For choosing the optimal kernel function\n",
      "\n",
      "Current kernel:  poly\n",
      "F1 score:  0.40958825673060983\n",
      "========================================\n",
      "Current kernel:  rbf\n",
      "F1 score:  0.4232442372826927\n",
      "========================================\n",
      "Current kernel:  sigmoid\n",
      "F1 score:  0.26266681223909444\n",
      "========================================\n",
      "Optimal kerenel function:  rbf\n",
      "F1 score: 0.423244\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Optimal regularization parameter:  100.0\n",
      "Optimal kernel:  rbf\n",
      "3-fold cross validation score: 0.465432\n"
     ]
    }
   ],
   "source": [
    "# Training your support vector machine classifier, and print out your validation(or cross-validation) error.\n",
    "# Save your own model\n",
    "# Your Code Here\n",
    "X_train = X_train_ohv.copy()\n",
    "\n",
    "reg_params = [0.001, 0.01, 0.05, 0.1, 1.0, 5.0, 10.0, 50.0, 100.0, 1000.0]\n",
    "max_f1 = 0\n",
    "\n",
    "print(\"For choosing the optimal regularization parameter\\n\")\n",
    "for param in reg_params:\n",
    "    f1_cv = []\n",
    "    svc = SVC(C=param)\n",
    "    for t_index, v_index in cv.split(X_train):\n",
    "        X_tr, y_tr = X_train[t_index], y_train[t_index]\n",
    "        X_val, y_val = X_train[v_index], y_train[v_index]\n",
    "        svc.fit(X_tr, y_tr)\n",
    "        y_pred = svc.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "        f1_cv.append(f1)\n",
    "    if max_f1 < np.mean(f1_cv):\n",
    "        max_f1 = np.mean(f1_cv)\n",
    "        opt_param = param\n",
    "    print(\"Regularization parameter: \", param)\n",
    "    print(\"F1 score: \", np.mean(f1_cv))\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"Optimal regularization parameter: \", opt_param)\n",
    "print(\"F1 score: {:.6f}\".format(max_f1))\n",
    "print(\"\\n\\n\\n\")    \n",
    "    \n",
    "kernels = ['poly', 'rbf', 'sigmoid']\n",
    "max_f1 = 0\n",
    "\n",
    "print(\"For choosing the optimal kernel function\\n\")\n",
    "for kernel in kernels:\n",
    "    f1_cv = []\n",
    "    svc = SVC(C=opt_param, kernel=kernel)\n",
    "    for t_index, v_index in cv.split(X_train):\n",
    "        X_tr, y_tr = X_train[t_index], y_train[t_index]\n",
    "        X_val, y_val = X_train[v_index], y_train[v_index]\n",
    "        svc.fit(X_tr, y_tr)\n",
    "        y_pred = svc.predict(X_val)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "        f1_cv.append(f1)\n",
    "    if max_f1 < np.mean(f1_cv):\n",
    "        max_f1 = np.mean(f1_cv)\n",
    "        opt_kernel = kernel\n",
    "    print(\"Current kernel: \", kernel)\n",
    "    print(\"F1 score: \", np.mean(f1_cv))\n",
    "    print(\"=\"*40)\n",
    "print(\"Optimal kerenel function: \", opt_kernel)\n",
    "print(\"F1 score: {:.6f}\".format(max_f1))\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "print(\"Optimal regularization parameter: \", opt_param)    \n",
    "print(\"Optimal kernel: \", opt_kernel)   \n",
    "print(\"3-fold cross validation score: {:.6f}\".format(cross_val_score(\\\n",
    "                            SVC(C=opt_param, kernel=opt_kernel), X_train, y_train, cv=3).mean()))\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Option) Other Classifiers.\n",
    "Train and validate other classifiers by your own manner.\n",
    "> <b> If you need, you can import other models only in this cell, only in scikit-learn. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current model for bagging:  Logistic Regression \n",
      "\n",
      "With 5 models, F1 score is 0.194204\n",
      "Accuracy: 0.257000\n",
      "========================================\n",
      "With 10 models, F1 score is 0.187815\n",
      "Accuracy: 0.252000\n",
      "========================================\n",
      "With 15 models, F1 score is 0.195679\n",
      "Accuracy: 0.263000\n",
      "========================================\n",
      "With 20 models, F1 score is 0.186904\n",
      "Accuracy: 0.253500\n",
      "========================================\n",
      "With 25 models, F1 score is 0.193956\n",
      "Accuracy: 0.257500\n",
      "========================================\n",
      "With 30 models, F1 score is 0.190547\n",
      "Accuracy: 0.257500\n",
      "========================================\n",
      "With 35 models, F1 score is 0.189012\n",
      "Accuracy: 0.256000\n",
      "========================================\n",
      "With 40 models, F1 score is 0.190781\n",
      "Accuracy: 0.260000\n",
      "========================================\n",
      "Optimal number of models:  15\n",
      "3-fold cross validation score: 0.274439\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current model for bagging:  Decision Tree \n",
      "\n",
      "With 5 models, F1 score is 0.429073\n",
      "Accuracy: 0.466000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 10 models, F1 score is 0.452904\n",
      "Accuracy: 0.481500\n",
      "========================================\n",
      "With 15 models, F1 score is 0.468887\n",
      "Accuracy: 0.495000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20 models, F1 score is 0.454635\n",
      "Accuracy: 0.485000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 25 models, F1 score is 0.479146\n",
      "Accuracy: 0.500500\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 30 models, F1 score is 0.477688\n",
      "Accuracy: 0.501500\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 35 models, F1 score is 0.459558\n",
      "Accuracy: 0.499000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 40 models, F1 score is 0.466857\n",
      "Accuracy: 0.504500\n",
      "========================================\n",
      "Optimal number of models:  25\n",
      "3-fold cross validation score: 0.501940\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current model for bagging:  Random Forest \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 5 models, F1 score is 0.433194\n",
      "Accuracy: 0.472000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 10 models, F1 score is 0.462866\n",
      "Accuracy: 0.480000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 15 models, F1 score is 0.445912\n",
      "Accuracy: 0.483000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20 models, F1 score is 0.455900\n",
      "Accuracy: 0.480000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 25 models, F1 score is 0.464734\n",
      "Accuracy: 0.486000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 30 models, F1 score is 0.472419\n",
      "Accuracy: 0.492000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 35 models, F1 score is 0.462846\n",
      "Accuracy: 0.488000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 40 models, F1 score is 0.451946\n",
      "Accuracy: 0.481000\n",
      "========================================\n",
      "Optimal number of models:  30\n",
      "3-fold cross validation score: 0.491945\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current model for bagging:  Support Vector Machine \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 5 models, F1 score is 0.405933\n",
      "Accuracy: 0.444500\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 10 models, F1 score is 0.419313\n",
      "Accuracy: 0.455500\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 15 models, F1 score is 0.408988\n",
      "Accuracy: 0.452500\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20 models, F1 score is 0.433607\n",
      "Accuracy: 0.459500\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 25 models, F1 score is 0.424960\n",
      "Accuracy: 0.460000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 30 models, F1 score is 0.415550\n",
      "Accuracy: 0.462000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 35 models, F1 score is 0.419730\n",
      "Accuracy: 0.456000\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\SJH\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 40 models, F1 score is 0.424578\n",
      "Accuracy: 0.466000\n",
      "========================================\n",
      "Optimal number of models:  20\n",
      "3-fold cross validation score: 0.460479\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If you need additional packages, import your own packages below.\n",
    "# Your Code Here\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "models = {\"Logistic Regression\" : LogisticRegression(C=0.1, solver='lbfgs', max_iter=2000, multi_class='multinomial'),\n",
    "         \"Decision Tree\" : DecisionTreeClassifier(criterion=\"entropy\", random_state=0),\n",
    "         \"Random Forest\" : RandomForestClassifier(n_estimators=50, criterion=\"entropy\", random_state=0),\n",
    "         \"Support Vector Machine\" : SVC(C=100.0, kernel=\"rbf\")}\n",
    "num_set = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "\n",
    "for model in models.keys():\n",
    "    print(\"\\nCurrent model for bagging: \", model, \"\\n\")\n",
    "    \n",
    "    max_f1 = 0\n",
    "    \n",
    "    if model == \"Logistic Regression\" or model == \"Support Vector Machine\":\n",
    "        X_train = X_train_ohv\n",
    "    else:\n",
    "        X_train = X_data\n",
    "        \n",
    "    for n in num_set:\n",
    "        f1_cv = []\n",
    "        accuracy_cv = []\n",
    "        bag = BaggingClassifier(base_estimator=models[model], n_estimators=n)\n",
    "        for t_index, v_index in cv.split(X_train):\n",
    "            X_tr, y_tr = X_train[t_index], y_train[t_index]\n",
    "            X_val, y_val = X_train[v_index], y_train[v_index]\n",
    "            \n",
    "            bag.fit(X_tr, y_tr)\n",
    "            y_pred = bag.predict(X_val)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "            f1_cv.append(f1)\n",
    "            \n",
    "            accuracy = len(y_pred[y_pred == y_val]) / len(y_pred)\n",
    "            accuracy_cv.append(accuracy)\n",
    "            \n",
    "        if max_f1 < np.mean(f1_cv):\n",
    "            max_f1 = np.mean(f1_cv)\n",
    "            opt_n = n\n",
    "            \n",
    "        print(\"With {} models, F1 score is {:.6f}\".format(n, np.mean(f1_cv)))\n",
    "        print(\"Accuracy: {:.6f}\".format(np.mean(accuracy_cv)))\n",
    "        print(\"=\"*40)\n",
    "    print(\"Optimal number of models: \", opt_n)\n",
    "    print(\"3-fold cross validation score: {:.6f}\".format(cross_val_score(BaggingClassifier(base_estimator=models[model], n_estimators=opt_n), X_train, y_train, cv=3).mean()))\n",
    "    print(\"\\n\\n\\n\")\n",
    "#End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit your prediction on the test data.\n",
    "\n",
    "* Select your model and explain it briefly.\n",
    "* You should read <b>\"test.csv\"</b>.\n",
    "* Prerdict your model with dictionary form.\n",
    "* Prediction example <br>\n",
    "[2, 6, 14, 8, $\\cdots$]\n",
    "* We will rank your result by <b>F1 metric(with 'macro' option)</b>.\n",
    "* <b> If you don't submit prediction file or submit it in wrong format, you can't get the point for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Explain your final model\n",
    "    parameter  F1 score      ,   ensemble   bagging  . bagging  , target class             stable   . bagging         .   Decision Tree  40    bagging       , Decision Tree Randoom Forest  categorical feature one-hot encoding    encoding        one-hot encoding   .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset.\n",
    "# Your Code Here\n",
    "X_test = pd.read_csv('./data/test.csv')\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "le = LabelEncoder()\n",
    "X_test[:,0], X_test[:,2], X_test[:,4] = map(le.fit_transform, [X_test[:,0], X_test[:,2], X_test[:,4]])\n",
    "\n",
    "numeric_features = [1, 3, 5]\n",
    "\n",
    "for feature_num in numeric_features:\n",
    "    feature_values = X_test[:, feature_num]\n",
    "    feature_mean = np.mean(feature_values)\n",
    "    feature_std = np.std(feature_values)\n",
    "    \n",
    "    X_test[:, feature_num] = (X_test[:, feature_num] - feature_mean) / feature_std\n",
    "    \n",
    "X_train = X_data.copy()\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target class\n",
    "# Make variable \"my_answer\", type of array, and fill this array with your class predictions.\n",
    "# Modify file name into your student number and your name.\n",
    "# Your Code Here\n",
    "bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy', random_state=0), n_estimators=25)\n",
    "my_answer = bag.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "file_name = \"HW2_2012130730_ShimJaeheon.csv\"\n",
    "# End Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is for saving predicted answers. DO NOT MODIFY.\n",
    "pd.Series(my_answer).to_csv(\"./data/\" + file_name, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
